{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fbbcfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jlegt\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hier werden die Bibs importiert\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import pmdarima as pm\n",
    "import itertools\n",
    "import statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "\n",
    "# LSTM\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# deep learning\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Other\n",
    "from IPython.display import Image\n",
    "from time import time\n",
    "import configparser\n",
    "import subprocess\n",
    "import warnings\n",
    "import pprint\n",
    "import os\n",
    "\n",
    "# Prophet\n",
    "from prophet import Prophet\n",
    "from prophet.plot import add_changepoints_to_plot\n",
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.diagnostics import performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "import holidays\n",
    "\n",
    "# machine learning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ee500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier werden die Anforderungen an den Anwender notiert\n",
    "'''\n",
    "csv-Datei; 2 Spalten; Datum, Name_Produkt\n",
    "Start- und Enddatum der csv ergeben den betrachteten Zeitraum, \n",
    "Datumsformat der csv als dd.mm.yyyy\n",
    "Aggloramation nur auf größere Zeiträume sinnvoll, keine Fehlerausgabe. Also von monatlichen Daten nicht auf Tagesdaten schließen\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9a39cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Zwichenspeicher\n",
    "# # # # \"C:\\Users\\jlegt\\Desktop\\Master\\Masterarbeit\\Programmierung\\Testdatensatz.csv\"; D, -\n",
    "# # # #  C:\\Users\\jlegt\\Desktop\\Master\\Masterarbeit\\Programmierung\\Test2.csv; M, .\n",
    "# # # # \"C:\\Users\\jlegt\\Desktop\\Master\\Masterarbeit\\Programmierung\\ind.csv\"\n",
    "# # # # Main Programm\n",
    "\n",
    "# Eingaben Anwender\n",
    "path, path_ind, decimal_separator, decimal_separator_ind, freq_timeseries, freq_forecast, agglo, fehler, selected_countries, horizont = eingaben()\n",
    "\n",
    "# Import Zeitreihe\n",
    "data, name_artikel = import_data(path, decimal_separator)\n",
    "\n",
    "# Import Anreicherung\n",
    "ind = import_ind(path_ind, decimal_separator_ind)\n",
    "\n",
    "# Aggloramation \n",
    "cumulated_values = cumulate_values(data, freq_forecast)\n",
    "\n",
    "# Datensatz in Trainings-, Validations- und Testdaten teilen\n",
    "train_data, valid_data, test_data, train_valid_data = split_data(cumulated_values, horizont)\n",
    "\n",
    "# Untersuchung Korrelation und merging Absatz und Indizes\n",
    "result_df = korrelation (cumulated_values, ind, horizont)\n",
    "\n",
    "# Datensatz mit Korrelationen in Trainings-, Validations- und Testdaten teilen\n",
    "train_korr, valid_korr, test_korr, train_valid_korr = split_data(result_df, horizont)\n",
    "\n",
    "# mehrere Diagramme der einzelnen Aggloramationen\n",
    "create_plots_single(data, freq_timeseries, name_artikel)\n",
    "\n",
    "# Diagramm der einzelnen Aggloramationen in einem Diagramm\n",
    "create_plots_one(data, freq_timeseries, name_artikel)\n",
    "\n",
    "# Zerlegung additiv\n",
    "seasonal_decomposition_add(cumulated_values, name_artikel, agglo)\n",
    "\n",
    "# Zerlegung multiplikativ\n",
    "marker = seasonal_decomposition_mul(cumulated_values, name_artikel, agglo)\n",
    "\n",
    "# ACF und PACF anzeigen\n",
    "acf(cumulated_values['value'], lags=20) \n",
    "\n",
    "# lineare Regression mit Fehlern des Validierungsbereiches \n",
    "linReg_4, linRegValid_mae, linRegValid_mse, linRegValid_rmse, linRegValid_mape, linRegValid_r2 = linReg80(train_data, valid_data, name_artikel, agglo)\n",
    "\n",
    "# lineare Regression mit Fehlern des Testbereiches\n",
    "linReg_5, linRegTest_mae, linRegTest_mse, linRegTest_rmse, linRegTest_mape, linRegTest_r2 = linReg80(train_valid_data, test_data, name_artikel, agglo)\n",
    "\n",
    "# Crosten mit Fehlern des Validierungsbereiches\n",
    "Cros_4, CrosValid_mae, CrosValid_mse, CrosValid_mape, CrosValid_r2, CrosValid_rmse, best_alpha = croston80(train_data, valid_data, fehler, name_artikel, agglo)\n",
    "print(best_alpha)\n",
    "# Crosten mit Fehlern Prognosehorizont\n",
    "Cros_5, CrosTest_mae, CrosTest_mse, CrosTest_mape, CrosTest_r2, CrosTest_rmse= croston100(train_valid_data, test_data, fehler, name_artikel, agglo, best_alpha)\n",
    "\n",
    "# Holt-Winters add und mul mit Fehlern des Validierungsbereiches\n",
    "if marker:\n",
    "    (HWMul_4, HWAdd_4, HWMulValid_mae, HWMulValid_mse, HWMulValid_mape, HWMulValid_r2, HWMulValid_rmse, HWAddValid_mae, HWAddValid_mse, HWAddValid_mape, HWAddValid_r2, HWAddValid_rmse) = HoWi80(train_data, valid_data, agglo, fehler, name_artikel, marker, freq_forecast)\n",
    "else:\n",
    "    (HWAdd_4, HWAddValid_mae, HWAddValid_mse, HWAddValid_mape, HWAddValid_r2, HWAddValid_rmse) = HoWi80(train_data, valid_data, agglo, fehler, name_artikel, marker, freq_forecast)\n",
    "\n",
    "# Holt-Winters add und mul mit Fehlern Prognosehorizont\n",
    "if marker:\n",
    "    (HWMul_5, HWAdd_5, HWMulTest_mae, HWMulTest_mse, HWMulTest_mape, HWMulTest_r2,HWMulTest_rmse, HWAddTest_mae, HWAddTest_mse, HWAddTest_mape, HWAddTest_r2, HWAddTest_rmse) = HoWi100(train_valid_data, test_data, agglo, fehler, name_artikel, marker, freq_forecast)\n",
    "else:\n",
    "    (HWAdd_5, HWAddTest_mae, HWAddTest_mse, HWAddTest_mape, HWAddTest_r2, HWAddTest_rmse) = HoWi100(train_valid_data, test_data, agglo, fehler, name_artikel, marker, freq_forecast)\n",
    "\n",
    "# Arima univariat mit Fehlern im Testbereich\n",
    "Arima_4, ArimaValid_mae, ArimaValid_mse, ArimaValid_mape, ArimaValid_r2, ArimaValid_rmse, order_ARIMA = ARIMA80(train_data, valid_data, fehler)\n",
    "\n",
    "# Arima univariat mit Fehlern Prognosehorizont\n",
    "Arima_5, ArimaTest_mae, ArimaTest_mse, ArimaTest_mape, ArimaTest_r2, ArimaTest_rmse = ARIMA100(train_valid_data, test_data, fehler, order_ARIMA)\n",
    "\n",
    "# Sarima univariat mit Fehlern im Testbereich\n",
    "Sarima_4, SarimaValid_mae, SarimaValid_mse, SarimaValid_mape, SarimaValid_r2, SarimaValid_rmse, order_SARIMA, seasonal_order_SARIMA = SARIMA80(train_data, valid_data, freq_forecast, fehler)\n",
    "\n",
    "# Sarima univariat mit Fehlern Prognosehorizont\n",
    "Sarima_5, SarimaTest_mae, SarimaTest_mse, SarimaTest_mape, SarimaTest_r2, SarimaTest_rmse = SARIMA100(train_valid_data, test_data, freq_forecast, fehler, order_SARIMA, seasonal_order_SARIMA)\n",
    "\n",
    "# Prophet univariat ohne Feiertage mit Fehlern im Testbereich\n",
    "FBPu_4, FBPuValid_mae, FBPuValid_mse, FBPuValid_mape, FBPuValid_r2, FBPuValid_rmse = prophet80uni(train_data, valid_data, freq_forecast)\n",
    "\n",
    "# Prophet univariat ohne Feiertage mit Fehlern Prognosehorizont\n",
    "FBPu_5, FBPuTest_mae, FBPuTest_mse, FBPuTest_mape, FBPuTest_r2, FBPuTest_rmse = prophet80uni(train_valid_data, test_data, freq_forecast)\n",
    "\n",
    "# LSTM multivariat mit Fehlern im Testbereich\n",
    "LSTM_4, LSTMValid_mae, LSTMValid_mse, LSTMValid_mape, LSTMValid_r2, LSTMValid_rmse = LSTM80(train_korr, valid_korr, fehler)\n",
    "\n",
    "# LSTM multivariat mit Fehlern Prognosehorizont\n",
    "LSTM_5, LSTMTest_mae, LSTMTest_mse, LSTMTest_mape, LSTMTest_r2, LSTMTest_rmse = LSTM80(train_valid_korr, test_korr, fehler)\n",
    "\n",
    "# RandomForrest mit Fehlern im Testbereich\n",
    "RF_4, RFValid_mae, RFValid_mse, RFValid_mape, RFValid_r2, RFValid_rmse = RF80(train_korr, valid_korr, fehler)\n",
    "\n",
    "# RandomForrest mit Fehlern Prognosehorizont\n",
    "RF_5, RFTest_mae, RFTest_mse, RFTest_mape, RFTest_r2, RFTest_rmse = RF80(train_valid_korr, test_korr, fehler)\n",
    "\n",
    "# SVM mit Fehlern im Testbereich\n",
    "SVM_4, SVMValid_mae, SVMValid_mse, SVMValid_mape, SVMValid_r2, SVMValid_rmse = SVM80(train_korr, valid_korr)\n",
    "\n",
    "# SVM mit Fehlern Prognosehorizont\n",
    "SVM_5, SVMTest_mae, SVMTest_mse, SVMTest_mape, SVMTest_r2, SVMTest_rmse = SVM80(train_valid_korr, test_korr)\n",
    "\n",
    "# Prophet mulitvariat mit Fehlern Testbereich\n",
    "FBPm_4, FBPmValid_mae, FBPmValid_mse, FBPmValid_mape, FBPmValid_r2, FBPmValid_rmse = prophet80multi(train_korr, valid_korr, selected_countries)\n",
    "\n",
    "# Prophet mulitvariat mit Fehlern Prognosehorizont\n",
    "FBPm_5, FBPmTest_mae, FBPmTest_mse, FBPmTest_mape, FBPmTest_r2, FBPmTest_rmse = prophet80multi(train_valid_korr, test_korr, selected_countries)\n",
    "\n",
    "# Backward Feature Elimination mittels LSTM\n",
    "(LSTM_b_4, LSTM_b_Valid_mae, LSTM_b_Valid_mse, LSTM_b_Valid_mape, \n",
    "LSTM_b_Valid_r2, LSTM_b_Valid_rmse, LSTM_b_model, LSTM_b_features)=backward_feature_elimination_valid(train_korr, valid_korr)\n",
    "\n",
    "# Backward Feature Elimination mittels LSTM Ergebnisforecast\n",
    "LSTM_b_5, LSTM_b_Test_mae, LSTM_b_Test_mse, LSTM_b_Test_mape, LSTM_b_Test_r2, LSTM_b_Test_rmse = backward_feature_forecast(train_valid_korr, test_korr, LSTM_b_model, LSTM_b_features)\n",
    "\n",
    "# Fehlermetriken der Validierungsbereiche der Modelle vergleichen\n",
    "ergebnis_valid(linRegValid_mae, linRegValid_mse, linRegValid_rmse, linRegValid_mape, linRegValid_r2,\n",
    "                 CrosValid_mae, CrosValid_mse, CrosValid_rmse, CrosValid_mape, CrosValid_r2,\n",
    "                 ArimaValid_mae, ArimaValid_mse, ArimaValid_rmse, ArimaValid_mape, ArimaValid_r2,\n",
    "                 SarimaValid_mae, SarimaValid_mse, SarimaValid_rmse, SarimaValid_mape, SarimaValid_r2,\n",
    "                 FBPuValid_mae, FBPuValid_mse, FBPuValid_rmse, FBPuValid_mape, FBPuValid_r2,\n",
    "                 LSTMValid_mae, LSTMValid_mse, LSTMValid_rmse, LSTMValid_mape, LSTMValid_r2,\n",
    "                 RFValid_mae, RFValid_mse, RFValid_rmse, RFValid_mape, RFValid_r2,\n",
    "                 SVMValid_mae, SVMValid_mse, SVMValid_rmse, SVMValid_mape, SVMValid_r2,\n",
    "                 FBPmValid_mae, FBPmValid_mse, FBPmValid_rmse, FBPmValid_mape, FBPmValid_r2,\n",
    "                 HWAddValid_mae, HWAddValid_mse, HWAddValid_rmse, HWAddValid_mape, HWAddValid_r2,\n",
    "                 HWMulValid_mae, HWMulValid_mse, HWMulValid_rmse, HWMulValid_mape, HWMulValid_r2,\n",
    "                 LSTM_b_Valid_mae, LSTM_b_Valid_mse, LSTM_b_Valid_rmse, LSTM_b_Valid_mape, LSTM_b_Valid_r2)\n",
    "\n",
    "# Fehlermetriken der Testbereiche der Modelle vergleichen\n",
    "ergebnis_test(linRegTest_mae, linRegTest_mse, linRegTest_rmse, linRegTest_mape, linRegTest_r2,\n",
    "                 CrosTest_mae, CrosTest_mse, CrosTest_rmse, CrosTest_mape, CrosTest_r2,\n",
    "                 ArimaTest_mae, ArimaTest_mse, ArimaTest_rmse, ArimaTest_mape, ArimaTest_r2,\n",
    "                 SarimaTest_mae, SarimaTest_mse, SarimaTest_rmse, SarimaTest_mape, SarimaTest_r2,\n",
    "                 FBPuTest_mae, FBPuTest_mse, FBPuTest_rmse, FBPuTest_mape, FBPuTest_r2,\n",
    "                 LSTMTest_mae, LSTMTest_mse, LSTMTest_rmse, LSTMTest_mape, LSTMTest_r2,\n",
    "                 RFTest_mae, RFTest_mse, RFTest_rmse, RFTest_mape, RFTest_r2,\n",
    "                 SVMTest_mae, SVMTest_mse, SVMTest_rmse, SVMTest_mape, SVMTest_r2,\n",
    "                 FBPmTest_mae, FBPmTest_mse, FBPmTest_rmse, FBPmTest_mape, FBPmTest_r2,\n",
    "                 HWAddTest_mae, HWAddTest_mse, HWAddTest_rmse, HWAddTest_mape, HWAddTest_r2,\n",
    "                 HWMulTest_mae, HWMulTest_mse, HWMulTest_rmse, HWMulTest_mape, HWMulTest_r2,\n",
    "                 LSTM_b_Test_mae, LSTM_b_Test_mse, LSTM_b_Test_rmse, LSTM_b_Test_mape, LSTM_b_Test_r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9164a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Feature Elimination mittels LSTM\n",
    "# Funktion zum Erstellen des LSTM-Modells\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential(name='lstm_vale3')\n",
    "    model.add(LSTM(units=input_shape[1], return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=10, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=10, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=10))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1))\n",
    "    return model\n",
    "\n",
    "def backward_feature_elimination_valid(train_korr, valid_korr):\n",
    "    train_copy = train_korr.copy()\n",
    "    valid_copy = valid_korr.copy()\n",
    "    train_copy.set_index('date', inplace=True)\n",
    "    valid_copy.set_index('date', inplace=True)\n",
    "    \n",
    "    if 'year' in train_copy.columns and 'month' in train_copy.columns and 'day' in train_copy.columns:\n",
    "        train_copy = train_copy.drop(['year', 'month', 'day'], axis=1)\n",
    "\n",
    "    if 'year' in valid_copy.columns and 'month' in valid_copy.columns and 'day' in valid_copy.columns:\n",
    "        valid_copy = valid_copy.drop(['year', 'month', 'day'], axis=1)\n",
    "\n",
    "    df = pd.concat([train_copy, valid_copy], ignore_index=True)\n",
    "    train_max = df.max()\n",
    "    train_min = df.min()\n",
    "    print(train_max)\n",
    "    train = (df - train_min) / (train_max - train_min)\n",
    "\n",
    "    size_train = len(train_copy)\n",
    "    df_train = train.iloc[:size_train]\n",
    "    df_valid = train.iloc[size_train:]\n",
    "\n",
    "    y_valid = df_valid['value'].values.astype('float32')\n",
    "    \n",
    "    best_model = None\n",
    "    best_features = None\n",
    "    best_metric = float('inf')\n",
    "    removed_features = []\n",
    "\n",
    "    available_features = list(df_train.columns)\n",
    "    available_features.remove('value')\n",
    "\n",
    "    while len(available_features) > 6:\n",
    "        current_best_metric = float('inf')\n",
    "        current_best_feature = None\n",
    "\n",
    "        for feature in available_features:\n",
    "            current_features = available_features.copy()\n",
    "            current_features.remove(feature)\n",
    "\n",
    "            model = create_lstm_model((1, len(current_features)))\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "            X_train = df_train[current_features].values.reshape(-1, 1, len(current_features))\n",
    "            y_train = df_train['value'].values.astype('float32')\n",
    "            X_valid = df_valid[current_features].values.reshape(-1, 1, len(current_features))\n",
    "            y_valid = df_valid['value'].values.astype('float32')\n",
    "\n",
    "            model.fit(X_train, y_train, epochs=10, batch_size=30, shuffle=False, validation_data=(X_valid, y_valid), verbose=0)\n",
    "            y_pred = model.predict(X_valid)\n",
    "\n",
    "            metric_value = mean_absolute_error(y_valid, y_pred)\n",
    "\n",
    "            if metric_value < current_best_metric:\n",
    "                current_best_metric = metric_value\n",
    "                current_best_feature = feature\n",
    "\n",
    "        if current_best_metric < best_metric:\n",
    "            best_metric = current_best_metric\n",
    "            best_features = current_features.copy()\n",
    "            available_features.remove(current_best_feature)\n",
    "            removed_features.append(current_best_feature)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Erstellen des endgültigen Modells mit den besten Merkmalen\n",
    "    best_model = create_lstm_model((1, len(best_features)))\n",
    "    best_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    X_train_final = df_train[best_features].values.reshape(-1, 1, len(best_features))\n",
    "    y_train_final = df_train['value'].values.astype('float32')\n",
    "    best_model.fit(X_train_final, y_train_final, epochs=100, batch_size=30, shuffle=False, verbose=0)\n",
    "\n",
    "    # Plotten des Verlaufs der Prognose des endgültigen Modells\n",
    "    y_pred_final = best_model.predict(X_train_final)\n",
    "\n",
    "    y_pred_final = y_pred_final*(train_max['value']-train_min['value'])+train_min['value']\n",
    "    y_train_final = y_train_final*(train_max['value']-train_min['value'])+train_min['value']\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "#     plt.plot(range(1, len(y_train_final) + 1), y_train_final, label='Trainingsdaten')\n",
    "#     plt.plot(range(2, len(y_train_final) + 1), y_pred_final[:-1], color='red', label='Vorhersagen (Train)')\n",
    "#     plt.title('LSTM Vorhersagen vs. Trainingsdaten')\n",
    "#     plt.xlabel('Datum')\n",
    "#     plt.ylabel('Wert')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "    # Kombination der besten 6 Features ausgeben\n",
    "    print(\"Beste Merkmalskombination:\", best_features)\n",
    "\n",
    "    # Entfernte Merkmale ausgeben\n",
    "    print(\"Entfernte Merkmale:\", removed_features)\n",
    "\n",
    "    # Gesamtes Modell ausgeben\n",
    "    print(\"Gesamtes Modell:\")\n",
    "    best_model.summary()\n",
    "\n",
    "    # Plot für den Forecast mit den Validierungsdaten\n",
    "    X_valid_final = df_valid[best_features].values.reshape(-1, 1, len(best_features))\n",
    "    y_pred_valid = best_model.predict(X_valid_final)\n",
    "\n",
    "    y_pred_valid = y_pred_valid*(train_max['value'] -train_min['value'])+train_min['value']\n",
    "    y_valid = y_valid*(train_max['value']-train_min['value'])+train_min['value']\n",
    "\n",
    "    # Plot für Trainingsdaten und Vorhersagen (Train)\n",
    "    plt.plot(range(1, len(y_train_final) + 1), y_train_final, color='blue', label='Trainingsdaten')\n",
    "    plt.plot(range(len(y_train_final), len(y_train_final) + len(y_valid)), y_valid, color='green', label='Validierungsdaten')\n",
    "    plt.plot(range(len(y_train_final), len(y_train_final) + len(y_pred_valid)), y_pred_valid, color='red', linestyle='--', label='Vorhersagen (Validierung)')\n",
    "    plt.plot(range(2, len(y_train_final) + 1), y_pred_final[:-1], color='orange', label='Vorhersagen (Train)')\n",
    "#     plt.title('LSTM Vorhersagen vs. Trainings- und Validierungsdaten')\n",
    "    plt.xlabel('Datum')\n",
    "    plt.ylabel('Wert')\n",
    "    plt.legend()\n",
    "    print(y_pred_valid)\n",
    "    # Speichern als PNG\n",
    "    plt.savefig('LSTM_BFS_train.png')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    \n",
    "    LSTM_b_Valid_mae = mean_absolute_error(y_valid, y_pred_valid)\n",
    "    LSTM_b_Valid_mse = mean_squared_error(y_valid, y_pred_valid)\n",
    "    LSTM_b_Valid_mape = mean_absolute_percentage_error(y_valid, y_pred_valid)\n",
    "    LSTM_b_Valid_r2 = r2_score(y_valid, y_pred_valid)\n",
    "    LSTM_b_Valid_rmse = np.sqrt(mean_squared_error(y_valid, y_pred_valid))\n",
    "                                \n",
    "    LSTM_b_model = best_model\n",
    "    LSTM_b_features = best_features\n",
    "    \n",
    "    return y_pred_valid, LSTM_b_Valid_mae, LSTM_b_Valid_mse, LSTM_b_Valid_mape, LSTM_b_Valid_r2, LSTM_b_Valid_rmse, LSTM_b_model, LSTM_b_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b302c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Feature Elimination mittels LSTM\n",
    "# Funktion zum Erstellen des LSTM-Modells\n",
    "def backward_feature_forecast(train_valid_korr, test_korr, LSTM_b_model, LSTM_b_features):\n",
    "    train_copy = train_valid_korr.copy()\n",
    "    valid_copy = test_korr.copy()\n",
    "    train_copy.set_index('date', inplace=True)\n",
    "    valid_copy.set_index('date', inplace=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    df = pd.concat([train_copy, valid_copy], ignore_index=True)\n",
    "    train_max = df.max()\n",
    "    train_min = df.min()\n",
    "#     print(train_max)\n",
    "    train = (df - train_min) / (train_max - train_min)\n",
    "\n",
    "    size_train = len(train_copy)\n",
    "    df_train = train.iloc[:size_train]\n",
    "    df_valid = train.iloc[size_train:]\n",
    "\n",
    "    y_valid = df_valid['value'].values.astype('float32')\n",
    "\n",
    "    best_features = LSTM_b_features\n",
    "    best_model = LSTM_b_model\n",
    "    \n",
    "    X_train_final = df_train[best_features].values.reshape(-1, 1, len(best_features))\n",
    "    y_train_final = df_train['value'].values.astype('float32')\n",
    "    best_model.fit(X_train_final, y_train_final, epochs=100, batch_size=30, shuffle=False, verbose=0)\n",
    "\n",
    "    # Plotten des Verlaufs der Prognose des endgültigen Modells\n",
    "    y_pred_final = best_model.predict(X_train_final)\n",
    "\n",
    "    y_pred_final = y_pred_final*(train_max['value']-train_min['value'])+train_min['value']\n",
    "    y_train_final = y_train_final*(train_max['value']-train_min['value'])+train_min['value']\n",
    "\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.plot(range(1, len(y_train_final) + 1), y_train_final, label='Trainingsdaten')\n",
    "#     plt.plot(range(2, len(y_train_final) + 1), y_pred_final[:-1], color='red', label='Vorhersagen (Train)')\n",
    "#     plt.title('LSTM Vorhersagen vs. Trainingsdaten')\n",
    "#     plt.xlabel('Datum')\n",
    "#     plt.ylabel('Wert')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    # Plot für den Forecast mit den Validierungsdaten\n",
    "    X_valid_final = df_valid[best_features].values.reshape(-1, 1, len(best_features))\n",
    "    y_pred_valid = best_model.predict(X_valid_final)\n",
    "\n",
    "    y_pred_valid = y_pred_valid*(train_max['value'] -train_min['value'])+train_min['value']\n",
    "    y_valid = y_valid*(train_max['value']-train_min['value'])+train_min['value']\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Plot für Trainingsdaten und Vorhersagen (Train)\n",
    "    plt.plot(range(1, len(y_train_final) + 1), y_train_final, color='blue', label='Trainingsdaten')\n",
    "    plt.plot(range(len(y_train_final), len(y_train_final) + len(y_valid)), y_valid, color='green', label='Validierungsdaten')\n",
    "    plt.plot(range(len(y_train_final), len(y_train_final) + len(y_pred_valid)), y_pred_valid, color='red', linestyle='--', label='Vorhersagen (Validierung)')\n",
    "    plt.plot(range(2, len(y_train_final) + 1), y_pred_final[:-1], color='orange', label='Vorhersagen (Train)')\n",
    "#     plt.title('LSTM Vorhersagen vs. Trainings- und Validierungsdaten')\n",
    "    plt.xlabel('Datum')\n",
    "    plt.ylabel('Wert')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    # Speichern als PNG\n",
    "    plt.savefig('LSTM_BFS_test.png')\n",
    "    plt.tight_layout()\n",
    "    print(y_pred_valid)\n",
    "    \n",
    "    LSTM_b_Test_mae = mean_absolute_error(y_valid, y_pred_valid)\n",
    "    LSTM_b_Test_mse = mean_squared_error(y_valid, y_pred_valid)\n",
    "    LSTM_b_Test_mape = mean_absolute_percentage_error(y_valid, y_pred_valid)\n",
    "    LSTM_b_Test_r2 = r2_score(y_valid, y_pred_valid)\n",
    "    LSTM_b_Test_rmse = np.sqrt(mean_squared_error(y_valid, y_pred_valid))\n",
    "                                \n",
    "\n",
    "    \n",
    "    return y_pred_valid, LSTM_b_Test_mae, LSTM_b_Test_mse, LSTM_b_Test_mape, LSTM_b_Test_r2, LSTM_b_Test_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab21764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eingaben():\n",
    "    # Eingaben Anwender; in eigenes Unterprogramm\n",
    "\n",
    "    # Pfad zur csv_Zeitreihe\n",
    "    path = str(input(\"Bitte geben Sie den Pfad zur CSV-Datei der Zeitreihe ein: \"))\n",
    "    # Pfad zur csv_Indizes\n",
    "    path_ind = str(input(\"Bitte geben Sie den Pfad zur CSV-Datei der Anreicherung ein: \"))\n",
    "    # Eingabe Dezimalzeichen Menge\n",
    "    decimal_separator = str(input(\"Bitte geben Sie das Dezimaltrennzeichen der CSV-Datei Zeitreihe ein (z.B.: '.' oder ','). Falls unbekannt, CSV-Datei via Text-Editor öffnen. Falls keins enthalten (bspw. Stückzahl) drücken Sie die Enter-Taste  \"))\n",
    "    # Eingabe Dezimalzeichen Menge\n",
    "    decimal_separator_ind = str(input(\"Bitte geben Sie das Dezimaltrennzeichen der CSV-Datei Anreicherung ein (z.B.: '.' oder ','). Falls unbekannt, CSV-Datei via Text-Editor öffnen. Falls keins enthalten (bspw. Stückzahl) drücken Sie die Enter-Taste  \"))\n",
    "    # Eingabe Aggloramation Zeiten\n",
    "    freq_timeseries = str(input(\"Bitte geben Sie die Aggloramation ein, in dem Ihre Daten angegeben sind (D, W, M, Q): \").upper())\n",
    "    # Welche Betrachtungsaggloramation\n",
    "    freq_forecast = str(input(\"Bitte geben Sie die Aggloramation ein, in dem Sie den Forecast betrachten wollen (D, W, M, Q): \").upper())\n",
    "    # Iteration nach welchem Fehler\n",
    "    fehler = str(input(\"Nach welchem Fehler soll iteriert werden (MAE, MSE, MAPE, R2, RMSE)? \").upper())\n",
    "    # Eingabeaufforderung für Länderkürzel Feiertage bei Prophet\n",
    "    selected_countries = input(\"Geben Sie die Länderkürzel durch Leerzeichen getrennt ein (z.B. DE US FRA): \").upper().split()\n",
    "    # Prognosehorizont festlegen\n",
    "    horizont = int(input(\"Wie viele Perioden soll der Forecast berechnen? \"))\n",
    "    \n",
    "    # Zuschneiden Importlink\n",
    "    path = path.strip('\"')\n",
    "    path_ind = path_ind.strip('\"')\n",
    "\n",
    "    # Überprüfung, ob die Eingabe leer ist\n",
    "    if not decimal_separator:  \n",
    "        decimal_separator = '.' \n",
    "    # Überprüfung, ob die Eingabe leer ist\n",
    "    if not decimal_separator_ind:  \n",
    "        decimal_separator_ind = '.' \n",
    "        \n",
    "    # Kontrolle Aggloramation Zeitreihe\n",
    "    valid_freq = ['D', 'W', 'M', 'Q']\n",
    "\n",
    "    if freq_timeseries in valid_freq:\n",
    "        print(\"Das eingegebene Zeitintervall ist:\", freq_timeseries) # kann raus\n",
    "    else:\n",
    "        print(\"Ungültiges Zeitintervall. Bitte geben Sie D, W, M oder Q ein.\")\n",
    "        sys.exit()\n",
    "\n",
    "    # Kontrolle Aggloramation Forecast\n",
    "    valid_freq = ['D', 'W', 'M', 'Q']\n",
    "\n",
    "    if freq_forecast in valid_freq:\n",
    "        print(\"Das eingegebene Zeitintervall ist:\", freq_forecast) # kann raus\n",
    "\n",
    "        # Festlegung der Variable agglo basierend auf freq_forecast\n",
    "        if freq_forecast == 'D':\n",
    "            agglo = 'täglich'\n",
    "        elif freq_forecast == 'W':\n",
    "            agglo = 'wöchentlich'\n",
    "        elif freq_forecast == 'M':\n",
    "            agglo = 'monatlich'\n",
    "        elif freq_forecast == 'Q':\n",
    "            agglo = 'quartalsweise'\n",
    "    else:\n",
    "        print(\"Ungültiges Zeitintervall. Bitte geben Sie D, W, M oder Q ein.\")\n",
    "        sys.exit()\n",
    "    return path, path_ind, decimal_separator, decimal_separator_ind, freq_timeseries, freq_forecast, agglo, fehler, selected_countries, horizont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92b275fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der Zeitreihe\n",
    "def import_data(path, decimal_separator):\n",
    "    possible_delimiters = [';', ':', '|', '\\t', ',']  # Liste der möglichen Trennzeichen\n",
    "    \n",
    "    for delimiter in possible_delimiters:\n",
    "        try:\n",
    "            data = pd.read_csv(path, delimiter=delimiter, decimal=decimal_separator)\n",
    "            # Wenn keine Fehler auftritt, breche die Schleife ab und gib die Daten zurück\n",
    "            break\n",
    "        except pd.errors.ParserError:\n",
    "            # Wenn ein Fehler auftritt, versuche das nächste Trennzeichen\n",
    "            continue\n",
    "    else:\n",
    "        # Falls kein passendes Trennzeichen gefunden wurde\n",
    "        print(\"Kein gültiges Trennzeichen gefunden.\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    data[data.columns[0]] = pd.to_datetime(data[data.columns[0]], format='%d.%m.%Y')\n",
    "    data = data.sort_values(data.columns[0])\n",
    "    \n",
    "    # Abfrage des Artikelnamens\n",
    "    if data is not None and len(data.columns) > 1:\n",
    "        name_artikel = data.columns[1]\n",
    "    else:\n",
    "        name_artikel = 'Artikel'\n",
    "    \n",
    "    # Umbenennung der Spalten\n",
    "    data = data.rename(columns={data.columns[0]: 'date', data.columns[1]: 'value'})\n",
    "    # Finde den Index des ersten Eintrags in der Spalte 'value', der nicht Null ist\n",
    "    first_nonzero_index = data['value'].ne(0).idxmax()\n",
    "\n",
    "    # Schneide den DataFrame ab dem ersten Eintrag, der nicht Null ist, bis zum Ende ab\n",
    "    data = data.loc[first_nonzero_index:]\n",
    "\n",
    "    return data, name_artikel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ecd1057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Anreicherung\n",
    "\n",
    "def import_ind(path, decimal_separator):\n",
    "    possible_delimiters = [';', ':', '|', '\\t', ',']  # Liste der möglichen Trennzeichen\n",
    "    \n",
    "    for delimiter in possible_delimiters:\n",
    "        try:\n",
    "            ind = pd.read_csv(path, delimiter=delimiter, decimal=decimal_separator)\n",
    "            # Wenn keine Fehler auftritt, breche die Schleife ab und gib die Daten zurück\n",
    "            break\n",
    "        except pd.errors.ParserError:\n",
    "            # Wenn ein Fehler auftritt, versuche das nächste Trennzeichen\n",
    "            continue\n",
    "    else:\n",
    "        # Falls kein passendes Trennzeichen gefunden wurde\n",
    "        print(\"Kein gültiges Trennzeichen gefunden.\")\n",
    "        return None\n",
    "    \n",
    "    ind[ind.columns[0]] = pd.to_datetime(ind[ind.columns[0]], format='%d.%m.%Y')\n",
    "    ind = ind.sort_values(ind.columns[0])\n",
    "    \n",
    "    # Umbenennung der Spalten\n",
    "    ind = ind.rename(columns={ind.columns[0]: 'date_ind'})\n",
    "    \n",
    "    return (ind)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cea13091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggloramation der zu betrachtenden Periodengröße\n",
    "\n",
    "def cumulate_values(data, freq_forecast):\n",
    "    # Start- und Enddatum der Zeitreihe identifizieren\n",
    "    start_date = data['date'].min()\n",
    "    end_date = data['date'].max()\n",
    "    \n",
    "    # Erstellen einer leeren Tabelle mit den Tagen von start_date bis end_date\n",
    "    dates = pd.date_range(start_date, end_date, freq='D')\n",
    "    days = pd.DataFrame({'date': dates, 'value': 0})\n",
    "    \n",
    "    # Hinzufügen der Anzahl für jeden Tag des aktuellen Artikels\n",
    "    for date in dates:\n",
    "        datum = date.date()\n",
    "        anzahl_sum = data.loc[data['date'].dt.date == pd.Timestamp(date).date(), 'value'].sum()\n",
    "        if anzahl_sum > 0:\n",
    "            days.loc[days['date'] == date, 'value'] = anzahl_sum\n",
    "            \n",
    "    if freq_forecast == 'D':\n",
    "        cumulated_values = days\n",
    "    elif freq_forecast == 'W':\n",
    "        cumulated_values = days.groupby(pd.Grouper(key='date', freq='W-MON')).sum().reset_index()\n",
    "    elif freq_forecast == 'M':\n",
    "        cumulated_values = days.groupby(pd.Grouper(key='date', freq='MS')).sum().reset_index()\n",
    "    elif freq_forecast == 'Q':\n",
    "        cumulated_values = days.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()\n",
    "    \n",
    "    return cumulated_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c2fe186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz in Trainings- und Testdaten teilen\n",
    "\n",
    "def split_data(data, horizont):\n",
    "    # Abtrennen der letzten 'horizont' Spalten als test_data\n",
    "    test_data = data.iloc[-horizont:]\n",
    "    \n",
    "    # Aufteilung der verbleibenden Daten in train_data und valid_data\n",
    "    train_valid_data = data.iloc[:-horizont]\n",
    "    train_size = int(len(train_valid_data) * 0.8)  # 80% der Daten für das Training\n",
    "    train_data = train_valid_data.iloc[:train_size]\n",
    "    valid_data = train_valid_data.iloc[train_size:]\n",
    "    \n",
    "    return train_data, valid_data, test_data, train_valid_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8a424d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untersuchung möglicher Korrelationen und anschließender Aneinanderfügung\n",
    "\n",
    "def korrelation (cumulated_values, ind, horizont):\n",
    "\n",
    "#     merged_df = pd.merge(cumulated_values, ind, left_on='date', right_on='date_ind', how='left')\n",
    "    merged_df = pd.merge(cumulated_values, ind, left_on='date', right_on='date_ind', how='outer')\n",
    "    # Sortieren nach Datum\n",
    "    merged_df = merged_df.sort_values('date_ind')\n",
    "    # Löschen der Spalte 'date_ind'\n",
    "    merged_df.drop(columns=['date_ind'], inplace=True)\n",
    "\n",
    "#     print(merged_df)\n",
    "    \n",
    "    # Separieren von verk (zweite Spalte) und anr (Spalten danach)\n",
    "    verk = merged_df.iloc[:, 1:2]  # die zweite Spalte\n",
    "    anr = merged_df.iloc[:, 1:]    # alle Spalten ab der dritten Spalte\n",
    "\n",
    "    # Anzahl der Lags, die erstellt werden sollen\n",
    "    num_lags = 24\n",
    "\n",
    "    # Erstellen des DataFrames für die Lag-Daten\n",
    "    lagged_anr = pd.DataFrame(index=range(horizont, len(anr)))  # Starten Sie bei 'horizont'\n",
    "\n",
    "    # Liste für den Output\n",
    "    output_list = []\n",
    "\n",
    "    # Berechnen der Korrelation zwischen der ersten Spalte von \"verk\" und den Lags für jede Spalte von \"anr\"\n",
    "    for column in anr.columns:\n",
    "        # DataFrame für die Lags der aktuellen Spalte erstellen, beginnend ab 'horizont'\n",
    "        lagged_column = pd.concat([anr[column].shift(i) for i in range(horizont, num_lags + 1)], axis=1)\n",
    "        lagged_column.columns = [f\"{column}_lag{i}\" for i in range(horizont, num_lags + 1)]\n",
    "\n",
    "        # Berechnen der Korrelation zwischen der ersten Spalte von \"verk\" und den Lags der aktuellen Spalte\n",
    "        correlations = {}\n",
    "        for lag_column in lagged_column.columns:\n",
    "            correlation = verk.iloc[:, 0].corr(lagged_column[lag_column].dropna())\n",
    "            correlations[lag_column] = correlation\n",
    "\n",
    "        # Den Lag mit der höchsten Korrelation für die aktuelle Spalte von \"anr\" finden\n",
    "        best_lag = max(correlations, key=correlations.get)\n",
    "        best_correlation = correlations[best_lag]\n",
    "\n",
    "        # Füge den Output zur Liste hinzu\n",
    "        output_list.append((column, best_lag, best_correlation))\n",
    "\n",
    "    # Sortieren der Liste nach den Korrelationen mit dem größten Betrag\n",
    "    sorted_output_list = sorted(output_list, key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "    # Behalte nur die ersten 15 Einträge der sortierten Liste\n",
    "    top_5_output_list = sorted_output_list[:15]\n",
    "\n",
    "    # Output anzeigen\n",
    "    print(\"Beste Lags für erste Spalte von 'verk' mit der entsprechenden Korrelation:\")\n",
    "    for item in top_5_output_list:\n",
    "        column, best_lag, correlation = item\n",
    "        print(f\"{column}: {best_lag} (Korrelation: {correlation})\")\n",
    "\n",
    "    # DataFrame für die Lags erstellen\n",
    "    lagged_df = pd.DataFrame()\n",
    "\n",
    "    # Durch top_5_output_list iterieren\n",
    "    for column_name, lag_string, _ in top_5_output_list:\n",
    "        # Den String zwischen den runden Klammern extrahieren\n",
    "        extracted_string = lag_string.split('(')[-1].split(')')[0]\n",
    "\n",
    "        # Den Namen der Originalspalte identifizieren\n",
    "        original_column_name = extracted_string.split('_lag')[0]\n",
    "\n",
    "        # Das Lag-Level extrahieren\n",
    "        lag_level = int(extracted_string.split('_lag')[-1])\n",
    "\n",
    "        # Das Lag berechnen\n",
    "        lagged_column = anr[original_column_name].shift(lag_level)\n",
    "\n",
    "        # Den Namen des generierten Lags erstellen\n",
    "        lagged_column_name = f\"{original_column_name}_lag{lag_level}\"\n",
    "\n",
    "        # Den generierten Lag dem DataFrame hinzufügen\n",
    "        lagged_df[lagged_column_name] = lagged_column\n",
    "\n",
    "    # Output anzeigen\n",
    "#     print(lagged_df)\n",
    "\n",
    "    result_df = pd.merge(verk, lagged_df, left_index=True, right_index=True)\n",
    "#     print(result_df)\n",
    "    # Hinzufügen der Spalte 'date' von merged_df zu results_df\n",
    "    result_df['date'] = merged_df['date']\n",
    "\n",
    "    # Spalte 'date' entfernen\n",
    "    date_column = result_df.pop('date')\n",
    "\n",
    "    # Spalte an die erste Position einfügen\n",
    "    result_df.insert(0, 'date', date_column)\n",
    "\n",
    "    # Entfernen der Zeilen mit NaN-Werten\n",
    "    result_df = result_df.dropna()\n",
    "\n",
    "    \n",
    "    # Extrahieren von Jahr, Monat und Tag und Erstellen der neuen Spalten\n",
    "    result_df['year'] = result_df['date'].dt.year\n",
    "    result_df['month'] = result_df['date'].dt.month\n",
    "    result_df['day'] = result_df['date'].dt.day\n",
    "    \n",
    "    print(result_df)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "75e05ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mehrere Diagramme der Einzelnen Aggloramationen\n",
    "\n",
    "def create_plots_single(cumulated_values, freq_timeseries, name_artikel):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    titles = {'D': 'Täglich', 'W': 'Wöchentlich', 'M': 'Monatlich', 'Q': 'Quartalsweise'}\n",
    "    \n",
    "    if freq_timeseries == 'D':\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(cumulated_values['date'], cumulated_values['value'], label='Täglich')\n",
    "        plt.xlabel('Datum')\n",
    "        plt.ylabel('Anzahl')\n",
    "#         plt.title('Tägliche Anzahl ' + name_artikel)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='W-MON')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='W-MON')).sum().reset_index()['value'], \n",
    "                 label='Wöchentlich')\n",
    "        plt.xlabel('Datum')\n",
    "        plt.ylabel('Anzahl')\n",
    "#         plt.title('Wöchentliche Anzahl ' + name_artikel)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='MS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='MS')).sum().reset_index()['value'], \n",
    "                 label='Monatlich')\n",
    "        plt.xlabel('Datum')\n",
    "        plt.ylabel('Anzahl')\n",
    "#         plt.title('Monatliche Anzahl ' + name_artikel)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['value'], \n",
    "                 label='Quartalsweise')\n",
    "        plt.xlabel('Datum')\n",
    "        plt.ylabel('Anzahl')\n",
    "#         plt.title('Quartalsweise Anzahl ' + name_artikel)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "\n",
    "    elif freq_timeseries == 'W':\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='W-MON')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='W-MON')).sum().reset_index()['value'], \n",
    "                 label='Wöchentlich')\n",
    "        plt.xlabel('Datum')\n",
    "        plt.ylabel('Anzahl')\n",
    "#         plt.title('Wöchentliche Anzahl ' + name_artikel)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='MS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='MS')).sum().reset_index()['value'], \n",
    "                 label='Monatlich')\n",
    "        plt.xlabel('Datum')\n",
    "        plt.ylabel('Anzahl')\n",
    "#         plt.title('Monatliche Anzahl ' + name_artikel)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['value'], \n",
    "                 label='Quartalsweise')\n",
    "        plt.xlabel('Datum')\n",
    "        plt.ylabel('Anzahl')\n",
    "#         plt.title('Quartalsweise Anzahl ' + name_artikel)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "\n",
    "    elif freq_timeseries == 'M':\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='MS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='MS')).sum().reset_index()['value'], \n",
    "                 label='Monatlich')\n",
    "        plt.xlabel('Datum')\n",
    "        plt.ylabel('Anzahl')\n",
    "#         plt.title('Monatliche Anzahl ' + name_artikel)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['value'], \n",
    "                 label='Quartalsweise')\n",
    "        plt.xlabel('Datum')\n",
    "        plt.ylabel('Anzahl')\n",
    "#         plt.title('Quartalsweise Anzahl ' + name_artikel)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "\n",
    "    elif freq_timeseries == 'Q':\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['value'], \n",
    "                 label='Quartalsweise')\n",
    "        plt.xlabel('Datum')\n",
    "        plt.ylabel('Absatz [Stück]')\n",
    "#         plt.title('Quartalsweise Anzahl ' + name_artikel)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Verläufe einzeln.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "109deedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagramm der einzelnen Aggloramationen in einem Diagramm\n",
    "\n",
    "def create_plots_one(cumulated_values, freq_timeseries, name_artikel):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    titles = {'D': 'Täglich', 'W': 'Wöchentlich', 'M': 'Monatlich', 'Q': 'Quartalsweise'}\n",
    "    \n",
    "    if freq_timeseries == 'D':\n",
    "        plt.plot(cumulated_values['date'], cumulated_values['value'], label='Täglich')\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='W-MON')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='W-MON')).sum().reset_index()['value'], \n",
    "                 label='Wöchentlich', linestyle='--')\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='MS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='MS')).sum().reset_index()['value'], \n",
    "                 label='Monatlich', linestyle='-.')\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['value'], \n",
    "                 label='Quartalsweise', linestyle=':')\n",
    "\n",
    "    elif freq_timeseries == 'W':\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='W-MON')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='W-MON')).sum().reset_index()['value'], \n",
    "                 label='Wöchentlich', linestyle='--')\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='MS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='MS')).sum().reset_index()['value'], \n",
    "                 label='Monatlich', linestyle='-.')\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['value'], \n",
    "                 label='Quartalsweise', linestyle=':')\n",
    "\n",
    "    elif freq_timeseries == 'M':\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='MS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='MS')).sum().reset_index()['value'], \n",
    "                 label='Monatlich', linestyle='-.')\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['value'], \n",
    "                 label='Quartalsweise', linestyle=':')\n",
    "\n",
    "    elif freq_timeseries == 'Q':\n",
    "        plt.plot(cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['date'],\n",
    "                 cumulated_values.groupby(pd.Grouper(key='date', freq='QS')).sum().reset_index()['value'], \n",
    "                 label='Quartalsweise')\n",
    "\n",
    "    plt.xlabel('Datum')\n",
    "    plt.ylabel('Absatz [Stück]')\n",
    "#     plt.title('Anzahl ' + name_artikel)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Verläufe zusammen.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4333c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeitreihenzerlegung mit dem multiplikativen Modell\n",
    "\n",
    "def seasonal_decomposition_mul(data, name_artikel, agglo):\n",
    "    # DataFrame mit DateTime-Index erstellen, ohne den ursprünglichen DataFrame zu ändern\n",
    "    data_copy = data.copy()\n",
    "    data_copy.index = pd.to_datetime(data_copy['date'])\n",
    "    \n",
    "    # Marker für Anwesenheit '0' values\n",
    "    marker = False\n",
    "    # Prüfen, ob der DataFrame keine Anzahl \"0\" enthält\n",
    "    if (data_copy[\"value\"] != 0).all():\n",
    "        print('Zerlegung multiplikativ')\n",
    "        # Marker für '0' values\n",
    "        marker = True\n",
    "        # Zerlegung der Zeitreihe mit model='mul'\n",
    "        decompose_result = seasonal_decompose(data_copy[\"value\"], model='mul')\n",
    "        \n",
    "        # Zerlegungsplots anzeigen und Titel setzen\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(8, 6))\n",
    "        decompose_result.observed.plot(ax=ax1)\n",
    "        ax1.set_title(\"Observed\")\n",
    "        decompose_result.trend.plot(ax=ax2)\n",
    "        ax2.set_title(\"Trend\")\n",
    "        decompose_result.seasonal.plot(ax=ax3)\n",
    "        ax3.set_title(\"Seasonal\")\n",
    "        decompose_result.resid.plot(ax=ax4)\n",
    "        ax4.set_title(\"Residual\")\n",
    "#         plt.suptitle(\"Zerlegung multiplikativ für \" + name_artikel + ' für ' + agglo)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.savefig('Zerlegung mul.png')\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        print(\"Der DataFrame enthält Anzahl '0', daher wird keine Zerlegung mit model='mul' durchgeführt.\")\n",
    "    \n",
    "    return marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df82e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeitreihenzerlegung mit dem additiven Modell\n",
    "\n",
    "\n",
    "def seasonal_decomposition_add(data, name_artikel, agglo):\n",
    "    # DataFrame mit DateTime-Index erstellen, ohne den ursprünglichen DataFrame zu ändern\n",
    "    data_copy = data.copy()\n",
    "    data_copy.index = pd.to_datetime(data_copy['date'])\n",
    "    print('Zerlegung additiv')\n",
    "    # Zerlegung der Zeitreihe mit model='add'\n",
    "    decompose_result = seasonal_decompose(data_copy[\"value\"], model='add')\n",
    "\n",
    "    # Zerlegungsplots anzeigen und Titel setzen\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(8, 6))\n",
    "    decompose_result.observed.plot(ax=ax1)\n",
    "    ax1.set_title(\"Observed\")\n",
    "    decompose_result.trend.plot(ax=ax2)\n",
    "    ax2.set_title(\"Trend\")\n",
    "    decompose_result.seasonal.plot(ax=ax3)\n",
    "    ax3.set_title(\"Seasonal\")\n",
    "    decompose_result.resid.plot(ax=ax4)\n",
    "    ax4.set_title(\"Residual\")\n",
    "#     plt.suptitle(\"Zerlegung additiv für \" + name_artikel + ' für ' + agglo)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig('Zerlegung add.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "15e8734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF + PACF bessere Abbildung\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "def acf(cumulated_values, lags):\n",
    "    # Plot der ACF und PACF\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 6))\n",
    "    \n",
    "    # Berechnung und Plot der ACF\n",
    "    plot_acf(cumulated_values, lags, ax=axes[0])\n",
    "    axes[0].set_title('a)', fontsize=12, loc='left')\n",
    "    axes[0].set_ylabel('Korrelationswert', fontsize=12)\n",
    "    axes[0].set_xlabel('Verzögerung [Periode]', fontsize=12)\n",
    "    \n",
    "    # Berechnung und Plot der PACF\n",
    "    plot_pacf(cumulated_values, lags, ax=axes[1])\n",
    "    axes[1].set_title('b)', fontsize=12, loc='left')\n",
    "    axes[1].set_ylabel('Korrelationswert', fontsize=12)\n",
    "    axes[1].set_xlabel('Verzögerung [Periode]', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ACF+PACF.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64d19c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linReg80(train_data, valid_data, name_artikel, agglo):\n",
    "    # def linReg80(train_data, valid_data, test_data, name_artikel, agglo):\n",
    "    train_dates = train_data['date']\n",
    "    valid_dates = valid_data['date']\n",
    "\n",
    "\n",
    "    # Extrahieren Sie die Features und Zielvariablen aus den Daten\n",
    "    Y_train = train_data[\"value\"].values\n",
    "    Y_valid = valid_data[\"value\"].values\n",
    "\n",
    "    # Extrahieren des Index als Features\n",
    "    X_train = train_data.index.values.reshape(-1, 1)\n",
    "    X_valid = valid_data.index.values.reshape(-1, 1)\n",
    "\n",
    "    # Lineare Regression für Trainingsdaten durchführen\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train, Y_train)\n",
    "\n",
    "    # Prognose für Validierungsdaten durchführen\n",
    "    y_valid_pred = lin_reg.predict(X_valid)\n",
    "\n",
    "    # Berechnung der Fehlerindikatoren für Validierungsdaten\n",
    "    linRegValid_mae = mean_absolute_error(Y_valid, y_valid_pred)\n",
    "    linRegValid_mse = mean_squared_error(Y_valid, y_valid_pred)\n",
    "    linRegValid_rmse = np.sqrt(linRegValid_mse)\n",
    "    linRegValid_mape = mean_absolute_percentage_error(Y_valid, y_valid_pred)\n",
    "    linRegValid_r2 = r2_score(Y_valid, y_valid_pred)\n",
    "\n",
    "    \n",
    "    # Berechnung der Fehlerindikatoren für Testdaten\n",
    "#     linRegTest_mae = mean_absolute_error(Y_test, y_test_pred)\n",
    "#     linRegTest_mse = mean_squared_error(Y_test, y_test_pred)\n",
    "#     linRegTest_rmse = np.sqrt(linRegTest_mse)\n",
    "#     linRegTest_mape = mean_absolute_percentage_error(Y_test, y_test_pred)\n",
    "#     linRegTest_r2 = r2_score(Y_test, y_test_pred)\n",
    "\n",
    "   # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (8, 6)\n",
    "\n",
    "    # Daten und Regression plotten\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Plot für Trainings-, Validierungs- und Testdaten\n",
    "    plt.plot(train_dates, Y_train, label='Trainingsdaten', color='blue')  # Hellblau für Trainingsdaten\n",
    "    plt.plot(valid_dates, Y_valid, label='Validierungsdaten (wahre Werte)', color='green')  # Orange für Validierungsdaten\n",
    "    plt.plot(valid_dates, y_valid_pred, linestyle='--', label='Validierungsprognose', color='red')  # Rot für Validierungsprognose\n",
    "    plt.xlabel('Datum', fontsize=fontsize)\n",
    "    plt.ylabel('Anzahl', fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize_legende)\n",
    "#     plt.title(\"Lineare Regression und Prognose für \" + name_artikel + \" für \" + agglo, fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Überprüfe und speichere das Diagramm mit angepasstem Dateinamen\n",
    "    filename = 'linReg_train.png'\n",
    "    if os.path.exists(filename):\n",
    "        filename = 'linReg_test.png'\n",
    "        print('Prognose mit linearer Regression:')\n",
    "        print(y_valid_pred)\n",
    "\n",
    "    plt.show()\n",
    "    print(y_valid_pred)\n",
    "\n",
    "#     print('Fehlermetriken für Validierungsdaten:')\n",
    "#     print('MAE: ', linRegValid_mae)\n",
    "#     print('MSE: ', linRegValid_mse)\n",
    "#     print('RMSE: ', linRegValid_rmse)\n",
    "#     print('MAPE: ', linRegValid_mape)\n",
    "#     print('R2: ', linRegValid_r2)\n",
    "#     print()\n",
    "\n",
    "#     print('Fehlermetriken für Testdaten:')\n",
    "#     print('MAE: ', linRegTest_mae)\n",
    "#     print('MSE: ', linRegTest_mse)\n",
    "#     print('RMSE: ', linRegTest_rmse)\n",
    "#     print('MAPE: ', linRegTest_mape)\n",
    "#     print('R2: ', linRegTest_r2)\n",
    "\n",
    "#     # Daten und Regression für Validierungsdaten plotten\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "\n",
    "#     # Plot für Validierungsdaten\n",
    "#     plt.plot(valid_dates, Y_valid, label='Validierungsdaten (wahre Werte)')\n",
    "#     plt.plot(valid_dates, y_valid_pred, linestyle='--', label='Validierungsprognose')\n",
    "#     plt.xlabel('Datum')\n",
    "#     plt.ylabel('Anzahl')\n",
    "#     plt.legend()\n",
    "#     plt.title(\"Lineare Regression und Prognose für \" + name_artikel + \" für \" + agglo + \" (Validierungsdaten)\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Daten und Regression für Testdaten plotten\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "\n",
    "#     # Plot für Testdaten\n",
    "#     plt.plot(test_dates, Y_test, label='Testdaten (wahre Werte)')\n",
    "#     plt.plot(test_dates, y_test_pred, linestyle='--', label='Testprognose')\n",
    "#     plt.xlabel('Datum')\n",
    "#     plt.ylabel('Anzahl')\n",
    "#     plt.legend()\n",
    "#     plt.title(\"Lineare Regression und Prognose für \" + name_artikel + \" für \" + agglo + \" (Testdaten)\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    return y_valid_pred, linRegValid_mae, linRegValid_mse, linRegValid_rmse, linRegValid_mape, linRegValid_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70c81f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Croston mit Alpha-Schleife auf Trainingsbereich\n",
    "\n",
    "# Croston Berechnung\n",
    "def croston(data, alpha):\n",
    "    # Erstelle eine Kopie des Datenrahmens, um die ursprünglichen Daten nicht zu verändern\n",
    "    train_data_copy = data.copy()\n",
    "    \n",
    "    # Transformiere die Eingabedaten in ein numpy-Array\n",
    "    d = np.array(train_data_copy['value'])\n",
    "    \n",
    "    cols = len(d)  # Länge des historischen Zeitraums\n",
    "\n",
    "    # level (a), periodicity(p) und forecast (f)\n",
    "    a, p, f = np.full((3, cols), np.nan)\n",
    "    q = 1  # Perioden seit der letzten Beobachtung der Nachfrage\n",
    "\n",
    "    # Initialisierung\n",
    "    first_occurrence = np.argmax(d > 0)\n",
    "    a[0] = d[first_occurrence]\n",
    "    p[0] = 1 + first_occurrence\n",
    "    f[0] = a[0] / p[0]\n",
    "\n",
    "    # Erstelle alle t+1 Prognosen\n",
    "    for t in range(0, cols - 1):  # Angepasster Schleifenbereich\n",
    "        if d[t] > 0:\n",
    "            a[t + 1] = alpha * d[t] + (1 - alpha) * a[t]\n",
    "            p[t + 1] = alpha * q + (1 - alpha) * p[t]\n",
    "            f[t + 1] = a[t + 1] / p[t + 1]\n",
    "            q = 1\n",
    "        else:\n",
    "            a[t + 1] = a[t]\n",
    "            p[t + 1] = p[t]\n",
    "            f[t + 1] = f[t]\n",
    "            q += 1\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\"Demand\": d, \"Forecast\": f, \"Period\": p, \"Level\": a, \"Error\": d - f})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Bestes Alpha iterativ suchen\n",
    "def find_optimal_alpha(data, fehler):\n",
    "    if fehler == 'MAE':\n",
    "        best_alpha = None\n",
    "        best_mae = float('inf')\n",
    "\n",
    "        # Durchlaufen aller Alpha-Werte im Bereich von 0 bis 1 in 0,01-Schritten\n",
    "        for alpha in np.arange(0, 1, 0.01):\n",
    "            # Prognose mit dem aktuellen Alpha-Wert durchführen\n",
    "            croston_result = croston(data, alpha)\n",
    "\n",
    "            # Berechnung des MAE für die Prognose\n",
    "            mae = mean_absolute_error(croston_result['Demand'], croston_result['Forecast'])\n",
    "\n",
    "            # Überprüfen, ob der aktuelle MAE besser ist als der bisher beste\n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                best_alpha = alpha\n",
    "\n",
    "        return best_alpha\n",
    "\n",
    "    elif fehler == 'MSE':\n",
    "        best_alpha = None\n",
    "        best_mse = float('inf')\n",
    "\n",
    "        # Durchlaufen aller Alpha-Werte im Bereich von 0 bis 1 in 0,01-Schritten\n",
    "        for alpha in np.arange(0, 1, 0.01):\n",
    "            # Prognose mit dem aktuellen Alpha-Wert durchführen\n",
    "            croston_result = croston(data, alpha)\n",
    "\n",
    "            # Berechnung des MAE für die Prognose\n",
    "            mse = mean_squared_error(croston_result['Demand'], croston_result['Forecast'])\n",
    "\n",
    "            # Überprüfen, ob der aktuelle MAE besser ist als der bisher beste\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_alpha = alpha\n",
    "\n",
    "        return best_alpha\n",
    "    \n",
    "    elif fehler == 'MAPE':\n",
    "        best_alpha = None\n",
    "        best_mape = float('inf')\n",
    "\n",
    "        # Durchlaufen aller Alpha-Werte im Bereich von 0 bis 1 in 0,01-Schritten\n",
    "        for alpha in np.arange(0, 1, 0.01):\n",
    "            # Prognose mit dem aktuellen Alpha-Wert durchführen\n",
    "            croston_result = croston(data, alpha)\n",
    "\n",
    "            # Berechnung des MAE für die Prognose\n",
    "            mape = mean_absolute_percentage_error(croston_result['Demand'], croston_result['Forecast'])\n",
    "\n",
    "            # Überprüfen, ob der aktuelle MAE besser ist als der bisher beste\n",
    "            if mape < best_mape:\n",
    "                best_mape = mape\n",
    "                best_alpha = alpha\n",
    "\n",
    "        return best_alpha\n",
    "    \n",
    "    elif fehler == 'R2':\n",
    "        best_alpha = None\n",
    "        best_r2 = float('inf')\n",
    "\n",
    "        # Durchlaufen aller Alpha-Werte im Bereich von 0 bis 1 in 0,01-Schritten\n",
    "        for alpha in np.arange(0, 1, 0.01):\n",
    "            # Prognose mit dem aktuellen Alpha-Wert durchführen\n",
    "            croston_result = croston(data, alpha)\n",
    "\n",
    "            # Berechnung des MAE für die Prognose\n",
    "            r2 = r2_score(croston_result['Demand'], croston_result['Forecast'])\n",
    "\n",
    "            # Überprüfen, ob der aktuelle MAE besser ist als der bisher beste\n",
    "            if r2 < best_r2:\n",
    "                best_r2 = r2\n",
    "                best_alpha = alpha\n",
    "\n",
    "        return best_alpha\n",
    "\n",
    "    elif fehler == 'RMSE':\n",
    "        best_alpha = None\n",
    "        best_rmse = float('inf')\n",
    "\n",
    "        # Durchlaufen aller Alpha-Werte im Bereich von 0 bis 1 in 0,01-Schritten\n",
    "        for alpha in np.arange(0, 1, 0.01):\n",
    "            # Prognose mit dem aktuellen Alpha-Wert durchführen\n",
    "            croston_result = croston(data, alpha)\n",
    "\n",
    "            # Berechnung des RMSE für die Prognose\n",
    "            rmse = np.sqrt(mean_squared_error(croston_result['Demand'], croston_result['Forecast']))\n",
    "\n",
    "            # Überprüfen, ob der aktuelle RMSE besser ist als der bisher beste\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_alpha = alpha\n",
    "\n",
    "        return best_alpha\n",
    "\n",
    "# Verknüpfung der Croston-Unterprogramme und Diagramm und Fehlerrechnung\n",
    "def croston80(train_data, test_data, fehler, name_artikel, agglo): \n",
    "    best_alpha = find_optimal_alpha(train_data, fehler) # Bestes Alpha finden\n",
    "    result_train = croston(train_data, best_alpha) # Ergebnis von croston80 mit dem besten Alpha für Trainingsdaten speichern\n",
    "#     result_test = croston(test_data, best_alpha) # Ergebnis von croston80 mit dem besten Alpha für Testdaten speichern\n",
    "    result_test = np.full_like(test_data['value'], result_train['Forecast'].iloc[-1])\n",
    "    \n",
    "    CrosTest_mae = mean_absolute_error(test_data['value'], result_test)\n",
    "    CrosTest_mse = mean_squared_error(test_data['value'], result_test)\n",
    "    CrosTest_mape = mean_absolute_percentage_error(test_data['value'], result_test)\n",
    "    CrosTest_r2 = r2_score(test_data['value'], result_test)\n",
    "    CrosTest_rmse = np.sqrt(mean_squared_error(test_data['value'], result_test))\n",
    "    # Plotten des Forecastes\n",
    "    \n",
    "    # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (8, 6)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Plot Trainingsdaten und Prognose für Trainingsdaten\n",
    "    plt.plot(train_data['date'], train_data['value'], label='Trainingsdaten', color='blue')\n",
    "    plt.plot(train_data['date'], result_train['Forecast'], label='Trainingsprognose', color='orange')\n",
    "\n",
    "    # Plot Testdaten und Prognose für Testdaten\n",
    "    plt.plot(test_data['date'], test_data['value'], label='Testdaten', color='green')\n",
    "    plt.plot(test_data['date'], result_test, label='Testprognose', linestyle='--', color='red')\n",
    "\n",
    "    plt.xlabel('Datum', fontsize=fontsize)\n",
    "    plt.ylabel('Wert', fontsize=fontsize)\n",
    "#     plt.title('Prognose nach Croston-Methode für Trainings- und Testdaten für ' + name_artikel + ' und ' + agglo, fontsize=fontsize)\n",
    "    plt.xticks(rotation=45, fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize_legende)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Exportiere das Diagramm als .png\n",
    "    plt.savefig('croston_train.png')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Output Fehler im Testintervall\n",
    "    \n",
    "#     print('MAE Valid: ', CrosTest_mae)\n",
    "#     print('MSE Valid: ', CrosTest_mse)\n",
    "#     print('MAPE Valid: ', CrosTest_mape)\n",
    "#     print('R2 Valid: ', CrosTest_r2)\n",
    "#     print('RMSE Valid: ', CrosTest_rmse)\n",
    "    return result_test, CrosTest_mae, CrosTest_mse, CrosTest_mape, CrosTest_r2, CrosTest_rmse, best_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ff5c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Croston mit Alpha-Schleife auf Trainingsbereich\n",
    "\n",
    "# Croston Berechnung\n",
    "def croston(data, alpha):\n",
    "    # Erstelle eine Kopie des Datenrahmens, um die ursprünglichen Daten nicht zu verändern\n",
    "    train_data_copy = data.copy()\n",
    "    \n",
    "    # Transformiere die Eingabedaten in ein numpy-Array\n",
    "    d = np.array(train_data_copy['value'])\n",
    "    \n",
    "    cols = len(d)  # Länge des historischen Zeitraums\n",
    "\n",
    "    # level (a), periodicity(p) und forecast (f)\n",
    "    a, p, f = np.full((3, cols), np.nan)\n",
    "    q = 1  # Perioden seit der letzten Beobachtung der Nachfrage\n",
    "\n",
    "    # Initialisierung\n",
    "    first_occurrence = np.argmax(d > 0)\n",
    "    a[0] = d[first_occurrence]\n",
    "    p[0] = 1 + first_occurrence\n",
    "    f[0] = a[0] / p[0]\n",
    "\n",
    "    # Erstelle alle t+1 Prognosen\n",
    "    for t in range(0, cols - 1):  # Angepasster Schleifenbereich\n",
    "        if d[t] > 0:\n",
    "            a[t + 1] = alpha * d[t] + (1 - alpha) * a[t]\n",
    "            p[t + 1] = alpha * q + (1 - alpha) * p[t]\n",
    "            f[t + 1] = a[t + 1] / p[t + 1]\n",
    "            q = 1\n",
    "        else:\n",
    "            a[t + 1] = a[t]\n",
    "            p[t + 1] = p[t]\n",
    "            f[t + 1] = f[t]\n",
    "            q += 1\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\"Demand\": d, \"Forecast\": f, \"Period\": p, \"Level\": a, \"Error\": d - f})\n",
    "    \n",
    "    return df\n",
    "\n",
    "def croston100(train_valid_data, test_data, fehler, name_artikel, agglo, best_alpha): \n",
    "    result_train = croston(train_valid_data, best_alpha) # Ergebnis von croston80 mit dem besten Alpha für Trainingsdaten speichern\n",
    "    \n",
    "    # Für den Testbereich einen konstanten Forecast erstellen\n",
    "    constant_forecast = np.full_like(test_data['value'], result_train['Forecast'].iloc[-1])\n",
    "    \n",
    "    print('Prognose für Croston:')\n",
    "    print(constant_forecast)\n",
    "    # Metriken für den konstanten Forecast berechnen\n",
    "    CrosTest_mae = mean_absolute_error(test_data['value'], constant_forecast)\n",
    "    CrosTest_mse = mean_squared_error(test_data['value'], constant_forecast)\n",
    "    CrosTest_mape = mean_absolute_percentage_error(test_data['value'], constant_forecast)\n",
    "    CrosTest_r2 = r2_score(test_data['value'], constant_forecast)\n",
    "    CrosTest_rmse = np.sqrt(mean_squared_error(test_data['value'], constant_forecast))\n",
    "\n",
    "    # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (8, 6)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Plot Trainingsdaten und Prognose für Trainingsdaten\n",
    "    plt.plot(train_valid_data['date'], train_valid_data['value'], label='Trainingsdaten', color='blue')\n",
    "    plt.plot(train_valid_data['date'], result_train['Forecast'], label='Trainingsprognose', color='orange')\n",
    "\n",
    "    # Plot Testdaten und konstanten Forecast für Testdaten\n",
    "    plt.plot(test_data['date'], test_data['value'], label='Testdaten', color='green')\n",
    "    plt.plot(test_data['date'], constant_forecast, label='Konstanter Forecast', linestyle='--', color='red')\n",
    "\n",
    "    plt.xlabel('Datum', fontsize=fontsize)\n",
    "    plt.ylabel('Wert', fontsize=fontsize)\n",
    "#     plt.title('Prognose nach Croston-Methode für Trainings- und Testdaten für ' + name_artikel + ' und ' + agglo, fontsize=fontsize)\n",
    "    plt.xticks(rotation=45, fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize_legende)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Exportiere das Diagramm als .png\n",
    "    plt.savefig('croston_forecast.png')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return constant_forecast, CrosTest_mae, CrosTest_mse, CrosTest_mape, CrosTest_r2, CrosTest_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02f76c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holt-Winters mit TestFehlern und Diagramm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def HoWi80(train_Data, test_Data, agglo, fehler, name_artikel, marker, freq_forecast):\n",
    "    \n",
    "    train_data = train_Data.copy()\n",
    "    test_data = test_Data.copy()\n",
    "\n",
    "    if freq_forecast == 'D':\n",
    "        periods = 365\n",
    "    elif freq_forecast == 'W':\n",
    "        periods = 52\n",
    "    elif freq_forecast == 'M':\n",
    "        periods = 12\n",
    "    elif freq_forecast == 'Q':\n",
    "        periods = 4\n",
    "    else:\n",
    "        print(\"Ungültiger Wert für 'freq_forecast'. Bitte wählen Sie 'D', 'W', 'M' oder 'Q'.\")\n",
    "        return\n",
    "\n",
    "    # Berechnen der Anzahl der Perioden\n",
    "    n_season = len(train_data) // periods if len(train_data) % periods == 0 else len(train_data) // periods\n",
    "\n",
    "    if n_season < 2:\n",
    "        print(\"Die Anzahl der Perioden ist zu klein für Holt-Winters.\")\n",
    "        print(\"Das Programm wird abgebrochen.\")\n",
    "        return\n",
    "\n",
    "    # Prüfen, ob der DataFrame Nullen enthält\n",
    "    if marker:\n",
    "        # Multiplikative Holt-Winters Exponential Smoothing (HWES)\n",
    "        fitted_model_mul = ExponentialSmoothing(train_data['value'], trend='mul', seasonal='mul', seasonal_periods=periods).fit()\n",
    "        test_predictions_mul = fitted_model_mul.predict(start=test_data.index[0], end=test_data.index[-1])\n",
    "        \n",
    "        \n",
    "#         # Berechnung der Trainings- und Testfehler für die Multiplikative Prognose\n",
    "#         mae_train_hw_mul = mean_absolute_error(train_data['value'], fitted_model_mul.fittedvalues)\n",
    "#         mse_train_hw_mul = mean_squared_error(train_data['value'], fitted_model_mul.fittedvalues)\n",
    "#         mape_train_hw_mul = mean_absolute_percentage_error(train_data['value'], fitted_model_mul.fittedvalues)\n",
    "#         r2_train_hw_mul = r2_score(train_data['value'], fitted_model_mul.fittedvalues)\n",
    "        \n",
    "        HWMulValid_mae = mean_absolute_error(test_data['value'], test_predictions_mul)\n",
    "        HWMulValid_mse = mean_squared_error(test_data['value'], test_predictions_mul)\n",
    "        HWMulValid_mape = mean_absolute_percentage_error(test_data['value'], test_predictions_mul)\n",
    "        HWMulValid_r2 = r2_score(test_data['value'], test_predictions_mul)\n",
    "        HWMulValid_rmse = np.sqrt(mean_squared_error(test_data['value'], test_predictions_mul))\n",
    "        \n",
    "        # Setze die Parameter gemäß den Vorgaben\n",
    "        fontsize = 12\n",
    "        fontsize_legende = 8\n",
    "        figsize = (8, 6)\n",
    "\n",
    "        # Plot der Multiplikativen Prognose\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.plot(train_data['date'], train_data['value'], label='Trainingsdaten', color='blue')\n",
    "        plt.plot(test_data['date'], test_data['value'], label='Testdaten', color='green')\n",
    "        plt.plot(train_data['date'], fitted_model_mul.fittedvalues, label='Modell Trainingsdaten', color='orange')\n",
    "        plt.plot(test_data['date'], fitted_model_mul.forecast(len(test_data)), linestyle='--', label='Prognose', color='red')\n",
    "        plt.xlabel('Datum', fontsize=fontsize)\n",
    "        plt.ylabel('Wert', fontsize=fontsize)\n",
    "#         plt.title('Multiplikative Holt-Winters Exponential Smoothing', fontsize=fontsize)\n",
    "        plt.legend(fontsize=fontsize_legende)\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Exportiere das Diagramm als .png\n",
    "        plt.savefig('HWMul_train.png')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"Der DataFrame für\", name_artikel, \"enthält Nullen, daher wird keine Zerlegung mit model='mul' durchgeführt.\")\n",
    "        print()\n",
    "\n",
    "    # Additive Holt-Winters Exponential Smoothing (HWES)\n",
    "    fitted_model_add = ExponentialSmoothing(train_data['value'], trend='add', seasonal='add', seasonal_periods=periods).fit()\n",
    "    test_predictions_add = fitted_model_add.predict(start=test_data.index[0], end=test_data.index[-1])\n",
    "\n",
    "#     # Berechnung der Trainings- und Testfehler für die Additive Prognose\n",
    "#     mae_train_hw_add = mean_absolute_error(train_data['value'], fitted_model_add.fittedvalues)\n",
    "#     mse_train_hw_add = mean_squared_error(train_data['value'], fitted_model_add.fittedvalues)\n",
    "#     mape_train_hw_add = mean_absolute_percentage_error(train_data['value'], fitted_model_add.fittedvalues)\n",
    "#     r2_train_hw_add = r2_score(train_data['value'], fitted_model_add.fittedvalues)\n",
    "\n",
    "    HWAddValid_mae = mean_absolute_error(test_data['value'], test_predictions_add)\n",
    "    HWAddValid_mse = mean_squared_error(test_data['value'], test_predictions_add)\n",
    "    HWAddValid_mape = mean_absolute_percentage_error(test_data['value'], test_predictions_add)\n",
    "    HWAddValid_r2 = r2_score(test_data['value'], test_predictions_add)\n",
    "    HWAddValid_rmse = np.sqrt(mean_squared_error(test_data['value'], test_predictions_add))\n",
    "    \n",
    "    # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (8, 6)\n",
    "\n",
    "    # Plot der Additiven Prognose\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(train_data['date'], train_data['value'], label='Trainingsdaten', color='blue')\n",
    "    plt.plot(test_data['date'], test_data['value'], label='Testdaten', color='green')\n",
    "    plt.plot(train_data['date'], fitted_model_add.fittedvalues, label='Modell Trainingsdaten', color='orange')\n",
    "    plt.plot(test_data['date'], fitted_model_add.forecast(len(test_data)), linestyle='--', label='Prognose', color='red')\n",
    "    plt.xlabel('Datum', fontsize=fontsize)\n",
    "    plt.ylabel('Wert', fontsize=fontsize)\n",
    "#     plt.title('Additive Holt-Winters Exponential Smoothing', fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize_legende)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Exportiere das Diagramm als .png\n",
    "    plt.savefig('HWAdd_Train.png')\n",
    "\n",
    "    plt.show()\n",
    "    if marker:\n",
    "        return (test_predictions_mul, test_predictions_add, HWMulValid_mae, HWMulValid_mse, HWMulValid_mape, HWMulValid_r2, HWMulValid_rmse, \n",
    "            HWAddValid_mae, HWAddValid_mse, HWAddValid_mape, HWAddValid_r2, HWAddValid_rmse)\n",
    "    else:\n",
    "        return (test_predictions_add, HWAddValid_mae, HWAddValid_mse, HWAddValid_mape, HWAddValid_r2, HWAddValid_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afe3efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holt-Winters mit TestFehlern und Diagramm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def HoWi100(train_Data, test_Data, agglo, fehler, name_artikel, marker, freq_forecast):\n",
    "    \n",
    "    train_data = train_Data.copy()\n",
    "    test_data = test_Data.copy()\n",
    "\n",
    "    if freq_forecast == 'D':\n",
    "        periods = 365\n",
    "    elif freq_forecast == 'W':\n",
    "        periods = 52\n",
    "    elif freq_forecast == 'M':\n",
    "        periods = 12\n",
    "    elif freq_forecast == 'Q':\n",
    "        periods = 4\n",
    "    else:\n",
    "        print(\"Ungültiger Wert für 'freq_forecast'. Bitte wählen Sie 'D', 'W', 'M' oder 'Q'.\")\n",
    "        return\n",
    "\n",
    "    # Berechnen der Anzahl der Perioden\n",
    "    n_season = len(train_data) // periods if len(train_data) % periods == 0 else len(train_data) // periods\n",
    "\n",
    "    if n_season < 2:\n",
    "        print(\"Die Anzahl der Perioden ist zu klein für Holt-Winters.\")\n",
    "        print(\"Das Programm wird abgebrochen.\")\n",
    "        return\n",
    "\n",
    "    # Prüfen, ob der DataFrame Nullen enthält\n",
    "    if marker:\n",
    "        # Multiplikative Holt-Winters Exponential Smoothing (HWES)\n",
    "        fitted_model_mul = ExponentialSmoothing(train_data['value'], trend='mul', seasonal='mul', seasonal_periods=periods).fit()\n",
    "        test_predictions_mul = fitted_model_mul.predict(start=test_data.index[0], end=test_data.index[-1])\n",
    "        \n",
    "        print('Prognose durch Holt-Winters multiplikativ:')\n",
    "        print(test_predictions_mul)\n",
    "        \n",
    "#         # Berechnung der Trainings- und Testfehler für die Multiplikative Prognose\n",
    "#         mae_train_hw_mul = mean_absolute_error(train_data['value'], fitted_model_mul.fittedvalues)\n",
    "#         mse_train_hw_mul = mean_squared_error(train_data['value'], fitted_model_mul.fittedvalues)\n",
    "#         mape_train_hw_mul = mean_absolute_percentage_error(train_data['value'], fitted_model_mul.fittedvalues)\n",
    "#         r2_train_hw_mul = r2_score(train_data['value'], fitted_model_mul.fittedvalues)\n",
    "        \n",
    "        HWMulTest_mae = mean_absolute_error(test_data['value'], test_predictions_mul)\n",
    "        HWMulTest_mse = mean_squared_error(test_data['value'], test_predictions_mul)\n",
    "        HWMulTest_mape = mean_absolute_percentage_error(test_data['value'], test_predictions_mul)\n",
    "        HWMulTest_r2 = r2_score(test_data['value'], test_predictions_mul)\n",
    "        HWMulTest_rmse = np.sqrt(mean_squared_error(test_data['value'], test_predictions_mul))\n",
    "        # Setze die Parameter gemäß den Vorgaben\n",
    "        fontsize = 12\n",
    "        fontsize_legende = 8\n",
    "        figsize = (8, 6)\n",
    "        # Plot der Multiplikativen Prognose\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.plot(train_data['date'], train_data['value'], label='Trainingsdaten', color='blue')\n",
    "        plt.plot(test_data['date'], test_data['value'], label='Testdaten', color='green')\n",
    "        plt.plot(train_data['date'], fitted_model_mul.fittedvalues, label='Modell Trainingsdaten', color='orange')\n",
    "        plt.plot(test_data['date'], fitted_model_mul.forecast(len(test_data)), linestyle='--', label='Prognose', color='red')\n",
    "        plt.xlabel('Datum', fontsize=fontsize)\n",
    "        plt.ylabel('Wert', fontsize=fontsize)\n",
    "#         plt.title('Multiplikative Holt-Winters Exponential Smoothing', fontsize=fontsize)\n",
    "        plt.legend(fontsize=fontsize_legende)\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Exportiere das Diagramm als .png\n",
    "        plt.savefig('HWMul_forecast.png')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"Der DataFrame für\", name_artikel, \"enthält Nullen, daher wird keine Zerlegung mit model='mul' durchgeführt.\")\n",
    "        print()\n",
    "\n",
    "    # Additive Holt-Winters Exponential Smoothing (HWES)\n",
    "    fitted_model_add = ExponentialSmoothing(train_data['value'], trend='add', seasonal='add', seasonal_periods=periods).fit()\n",
    "    test_predictions_add = fitted_model_add.predict(start=test_data.index[0], end=test_data.index[-1])\n",
    "\n",
    "    print('Prognsoe mit Holt-Winters additiv:')\n",
    "    print(test_predictions_add)\n",
    "#     # Berechnung der Trainings- und Testfehler für die Additive Prognose\n",
    "#     mae_train_hw_add = mean_absolute_error(train_data['value'], fitted_model_add.fittedvalues)\n",
    "#     mse_train_hw_add = mean_squared_error(train_data['value'], fitted_model_add.fittedvalues)\n",
    "#     mape_train_hw_add = mean_absolute_percentage_error(train_data['value'], fitted_model_add.fittedvalues)\n",
    "#     r2_train_hw_add = r2_score(train_data['value'], fitted_model_add.fittedvalues)\n",
    "\n",
    "    HWAddTest_mae = mean_absolute_error(test_data['value'], test_predictions_add)\n",
    "    HWAddTest_mse = mean_squared_error(test_data['value'], test_predictions_add)\n",
    "    HWAddTest_mape = mean_absolute_percentage_error(test_data['value'], test_predictions_add)\n",
    "    HWAddTest_r2 = r2_score(test_data['value'], test_predictions_add)\n",
    "    HWAddTest_rmse = np.sqrt(mean_squared_error(test_data['value'], test_predictions_add))\n",
    "    \n",
    "    # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (8, 6)\n",
    "\n",
    "    # Plot der Additiven Prognose\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(train_data['date'], train_data['value'], label='Trainingsdaten', color='blue')\n",
    "    plt.plot(test_data['date'], test_data['value'], label='Testdaten', color='green')\n",
    "    plt.plot(train_data['date'], fitted_model_add.fittedvalues, label='Modell Trainingsdaten', color='orange')\n",
    "    plt.plot(test_data['date'], fitted_model_add.forecast(len(test_data)), linestyle='--', label='Prognose', color='red')\n",
    "    plt.xlabel('Datum', fontsize=fontsize)\n",
    "    plt.ylabel('Wert', fontsize=fontsize)\n",
    "#     plt.title('Additive Holt-Winters Exponential Smoothing', fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize_legende)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Exportiere das Diagramm als .png\n",
    "    plt.savefig('HWAdd_forecast.png')\n",
    "\n",
    "    plt.show()\n",
    "    if marker:\n",
    "        return (test_predictions_mul, test_predictions_add, HWMulTest_mae, HWMulTest_mse, HWMulTest_mape, HWMulTest_r2, HWMulTest_rmse, \n",
    "            HWAddTest_mae, HWAddTest_mse, HWAddTest_mape, HWAddTest_r2, HWAddTest_rmse)\n",
    "    else:\n",
    "        return (test_predictions_add, HWAddTest_mae, HWAddTest_mse, HWAddTest_mape, HWAddTest_r2, HWAddTest_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6764dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arima mit Fehlern im Testbereich\n",
    "# Versuch Arima selbst zu optimieren auf Fehlermetriken\n",
    "\n",
    "def ARIMA80(train_data, test_data, fehler):\n",
    "    \n",
    "    # Arbeitskopie erstellen\n",
    "    train_copy = train_data.copy()\n",
    "    test_copy = test_data.copy()\n",
    "\n",
    "    # Define the p, d and q parameters to take any valve between 0 and 2\n",
    "    p = d = q = range(0, 3)\n",
    "\n",
    "    # Generate all different combinations of p, q and q triplets\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "\n",
    "    #     Hyperparameter tuning for SARIMA\n",
    "    #     Determing p, d, q combinations with MSE scores.\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for param in pdq:\n",
    "        try:\n",
    "            mod = statsmodels.tsa.arima.model.ARIMA(train_copy['value'],\n",
    "                                           order=param,\n",
    "                                           enforce_stationarity=False,\n",
    "                                           enforce_invertibility=False)\n",
    "\n",
    "            results = mod.fit()\n",
    "\n",
    "            if fehler == 'MAE':\n",
    "                metric_value = results.mae\n",
    "            elif fehler == 'MSE':\n",
    "                metric_value = results.mse\n",
    "            elif fehler == 'MAPE':\n",
    "                metric_value = results.mae\n",
    "            elif fehler == 'RMSE':\n",
    "                metric_value = results.mse\n",
    "            elif fehler == 'R2':\n",
    "                metric_value = results.mae\n",
    "            else:\n",
    "                raise ValueError(\"Ungültige Metrik angegeben.\")\n",
    "\n",
    "            metric_text = 'ARIMA{} - {}: {}'.format(param, fehler, metric_value)\n",
    "            rows.append([metric_value, metric_text])\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Modell ARIMA{param}: {e}\")\n",
    "            continue\n",
    "\n",
    "    metric_results = pd.DataFrame(rows, columns=[\"metric_value\", \"Params\"])\n",
    "    metric_results = metric_results.sort_values(by='metric_value', ascending=True)\n",
    "\n",
    "    # Extrahiere die Parameter aus der obersten Zeile von metric_results\n",
    "    top_params = metric_results.iloc[0]['Params']\n",
    "    print(top_params)\n",
    "    p, d, q = eval(top_params.split(' - ')[0].replace('ARIMA', ''))\n",
    "\n",
    "    # Erstelle und trainiere das ARIMA-Modell mit den extrahierten Parametern\n",
    "    model = statsmodels.tsa.arima.model.ARIMA(train_copy['value'],\n",
    "                                   order=(p, d, q),\n",
    "                                   enforce_stationarity=False,\n",
    "                                   enforce_invertibility=False)\n",
    "\n",
    "    results = model.fit()\n",
    "\n",
    "    # Modellbewertung mit Testdaten\n",
    "    fc = results.forecast(steps=len(test_copy))\n",
    "\n",
    "    # Modellbewertung mit Trainingsdaten\n",
    "    fc_train = results.forecast(steps=len(train_copy))\n",
    "\n",
    "    # Berechnen der Fehlermetriken für Testdaten\n",
    "    ArimaTest_mae = mean_absolute_error(test_copy['value'], fc)\n",
    "    ArimaTest_mse = mean_squared_error(test_copy['value'], fc)\n",
    "    ArimaTest_mape = mean_absolute_percentage_error(test_copy['value'], fc)\n",
    "    ArimaTest_r2 = r2_score(test_copy['value'], fc)\n",
    "    ArimaTest_rmse =  np.sqrt(mean_squared_error(test_copy['value'], fc))\n",
    "\n",
    "#     print(f\"Mean Absolute Error (MAE) for ARIMA data: {ArimaTest_mae}\")\n",
    "#     print(f\"Mean Squared Error (MSE) for ARIMA data: {ArimaTest_mse}\")\n",
    "#     print(f\"Mean Absolute Percentage Error (MAPE) for ARIMA data: {ArimaTest_mape}\")\n",
    "#     print(f\"R-squared (R^2) for ARIMA data: {ArimaTest_r2}\")\n",
    "#     print(f\"RMSE for ARIMA data: {ArimaTest_rmse}\")\n",
    "\n",
    "    # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (8, 6)\n",
    "\n",
    "    # Plot der Vorhersagen, Trainingsdaten und Testdaten\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(train_copy['date'], train_copy['value'], label='Trainingsdaten', color='blue')\n",
    "    plt.plot(test_copy['date'], test_copy['value'], label='Testdaten', color='green')\n",
    "#     plt.plot(train_copy['date'], fc_train, color='blue', label='Vorhersagen (Train)')\n",
    "    plt.plot(test_copy['date'], fc, color='red', linestyle='--', label='Vorhersagen (Test)')\n",
    "    plt.xlabel('Datum', fontsize=fontsize)\n",
    "    plt.ylabel('Wert', fontsize=fontsize)\n",
    "#     plt.title('ARIMA Vorhersagen vs. Trainings- und Testdaten', fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize_legende)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Exportiere das Diagramm als .png\n",
    "    plt.savefig('arima_train.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    order=(p, d, q)\n",
    "    return (fc, ArimaTest_mae, ArimaTest_mse, ArimaTest_mape, ArimaTest_r2, ArimaTest_rmse, order)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8c16284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arima mit Fehlern im Testbereich\n",
    "# Versuch Arima selbst zu optimieren auf Fehlermetriken\n",
    "\n",
    "def ARIMA100(train_data, test_data, fehler, order):\n",
    "    \n",
    "    # Arbeitskopie erstellen\n",
    "    train_copy = train_data.copy()\n",
    "    test_copy = test_data.copy()\n",
    "\n",
    "    \n",
    "    # Erstelle und trainiere das ARIMA-Modell mit den extrahierten Parametern\n",
    "    model = statsmodels.tsa.arima.model.ARIMA(train_copy['value'],\n",
    "                                   order=order,\n",
    "                                   enforce_stationarity=False,\n",
    "                                   enforce_invertibility=False)\n",
    "\n",
    "    results = model.fit()\n",
    "\n",
    "    # Modellbewertung mit Testdaten\n",
    "    fc = results.forecast(steps=len(test_copy))\n",
    "\n",
    "    print('Prognose mit ARIMA:')\n",
    "    print(fc)\n",
    "    # Modellbewertung mit Trainingsdaten\n",
    "    fc_train = results.forecast(steps=len(train_copy))\n",
    "\n",
    "    # Berechnen der Fehlermetriken für Testdaten\n",
    "    ArimaTest_mae = mean_absolute_error(test_copy['value'], fc)\n",
    "    ArimaTest_mse = mean_squared_error(test_copy['value'], fc)\n",
    "    ArimaTest_mape = mean_absolute_percentage_error(test_copy['value'], fc)\n",
    "    ArimaTest_r2 = r2_score(test_copy['value'], fc)\n",
    "    ArimaTest_rmse =  np.sqrt(mean_squared_error(test_copy['value'], fc))\n",
    "\n",
    "#     print(f\"Mean Absolute Error (MAE) for ARIMA data: {ArimaTest_mae}\")\n",
    "#     print(f\"Mean Squared Error (MSE) for ARIMA data: {ArimaTest_mse}\")\n",
    "#     print(f\"Mean Absolute Percentage Error (MAPE) for ARIMA data: {ArimaTest_mape}\")\n",
    "#     print(f\"R-squared (R^2) for ARIMA data: {ArimaTest_r2}\")\n",
    "#     print(f\"RMSE for ARIMA data: {ArimaTest_rmse}\")\n",
    "\n",
    "    # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (8, 6)\n",
    "\n",
    "    # Plot der Vorhersagen, Trainingsdaten und Testdaten\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(train_copy['date'], train_copy['value'], label='Trainingsdaten', color='blue')\n",
    "    plt.plot(test_copy['date'], test_copy['value'], label='Testdaten', color='green')\n",
    "#     plt.plot(train_copy['date'], fc_train, color='blue', label='Vorhersagen (Train)')\n",
    "    plt.plot(test_copy['date'], fc, color='red', linestyle='--', label='Vorhersagen (Test)')\n",
    "    plt.xlabel('Datum', fontsize=fontsize)\n",
    "    plt.ylabel('Wert', fontsize=fontsize)\n",
    "#     plt.title('ARIMA Vorhersagen vs. Trainings- und Testdaten', fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize_legende)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Exportiere das Diagramm als .png\n",
    "    plt.savefig('arima_predictions.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ARIMA_model = results\n",
    "    return (fc, ArimaTest_mae, ArimaTest_mse, ArimaTest_mape, ArimaTest_r2, ArimaTest_rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16626b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarima mit Fehlern im Testbereich\n",
    "# Versuch Sarima selbst zu optimieren mit Fehlermetriken\n",
    "\n",
    "\n",
    "def SARIMA80(train_data, test_data, freq_forecast, fehler):\n",
    "    \n",
    "    # Arbeitskopie erstellen\n",
    "    train_copy = train_data.copy()\n",
    "    test_copy = test_data.copy()\n",
    "\n",
    "\n",
    "    if freq_forecast == 'D':\n",
    "        periods = 365\n",
    "    elif freq_forecast == 'W':\n",
    "        periods = 52\n",
    "    elif freq_forecast == 'M':\n",
    "        periods = 12\n",
    "    elif freq_forecast == 'Q':\n",
    "        periods = 4\n",
    "\n",
    "    # Define the p, d and q parameters to take any valve between 0 and 2\n",
    "    p = d = q = range(0, 2)\n",
    "\n",
    "    # Generate all different combinations of p, q and q triplets\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "    # Generate all different combinations of seasonal p, q and q triplets\n",
    "    seasonal_pdq = [(x[0], x[1], x[2], periods) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "    #     Hyperparameter tuning for SARIMA\n",
    "    #     Determing p, d, q combinations with MSE scores.\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for param in pdq:\n",
    "        for param_seasonal in seasonal_pdq:\n",
    "            try:\n",
    "                mod = statsmodels.tsa.arima.model.ARIMA(train_copy['value'],\n",
    "                                               order=param,\n",
    "                                                seasonal_order=param_seasonal,\n",
    "                                               enforce_stationarity=False,\n",
    "                                               enforce_invertibility=False)\n",
    "\n",
    "                results = mod.fit()\n",
    "\n",
    "                if fehler == 'MAE':\n",
    "                    metric_value = results.mae\n",
    "                elif fehler == 'MSE':\n",
    "                    metric_value = results.mse\n",
    "                elif fehler == 'MAPE':\n",
    "                    metric_value = results.mae\n",
    "                elif fehler == 'RMSE':\n",
    "                    metric_value = results.mse\n",
    "                elif fehler == 'R2':\n",
    "                    metric_value = results.mae\n",
    "                else:\n",
    "                    raise ValueError(\"Ungültige Metrik angegeben.\")\n",
    "\n",
    "                metric_text = 'SARIMA{}x{}'.format(param,param_seasonal)\n",
    "                rows.append([metric_value, metric_text])\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Modell ARIMA{param}: {e}\")\n",
    "                continue\n",
    "\n",
    "    metric_results = pd.DataFrame(rows, columns=[\"metric_value\", \"Params\"])\n",
    "    metric_results = metric_results.sort_values(by='metric_value', ascending=True)\n",
    "\n",
    "    # Extrahiere die Parameter aus der obersten Zeile von metric_results\n",
    "    top_params = metric_results.iloc[0]['Params']\n",
    "    print(top_params)\n",
    "    # Splitten der Parameter für ARIMA und saisonale Komponente\n",
    "    arima_params, seasonal_params = top_params.split('x')\n",
    "\n",
    "    # Extrahiere ARIMA-Parameter\n",
    "    p, d, q = [int(x) for x in arima_params.replace('SARIMA', '').replace('(', '').replace(')', '').split(',')]\n",
    "\n",
    "    # Extrahiere saisonale Komponenten-Parameter\n",
    "    seasonal_p, seasonal_d, seasonal_q, periods = [int(x) for x in seasonal_params.replace('(', '').replace(')', '').split(',')]\n",
    "\n",
    "    # Erstelle und trainiere das ARIMA-Modell mit den extrahierten Parametern\n",
    "    model = statsmodels.tsa.arima.model.ARIMA(train_copy['value'],\n",
    "                                   order=(p, d, q),\n",
    "                                   seasonal_order=(seasonal_p, seasonal_d, seasonal_q, periods),\n",
    "                                   enforce_stationarity=False,\n",
    "                                   enforce_invertibility=False)\n",
    "\n",
    "    results = model.fit()\n",
    "\n",
    "    \n",
    "\n",
    "    # Modellbewertung mit Testdaten\n",
    "    fc = results.forecast(steps=len(test_copy))\n",
    "\n",
    "    # Modellbewertung mit Trainingsdaten\n",
    "    fc_train = results.forecast(steps=len(train_copy))\n",
    "\n",
    "    # Berechnen der Fehlermetriken für Testdaten\n",
    "    SarimaTest_mae = mean_absolute_error(test_copy['value'], fc)\n",
    "    SarimaTest_mse = mean_squared_error(test_copy['value'], fc)\n",
    "    SarimaTest_mape = mean_absolute_percentage_error(test_copy['value'], fc)\n",
    "    SarimaTest_r2 = r2_score(test_copy['value'], fc)\n",
    "    SarimaTest_rmse =  np.sqrt(mean_squared_error(test_copy['value'], fc))\n",
    "\n",
    "#     print(f\"Mean Absolute Error (MAE) for SARIMA data: {SarimaTest_mae}\")\n",
    "#     print(f\"Mean Squared Error (MSE) for SARIMA data: {SarimaTest_mse}\")\n",
    "#     print(f\"Mean Absolute Percentage Error (MAPE) for SARIMA data: {SarimaTest_mape}\")\n",
    "#     print(f\"R-squared (R^2) for SARIMA data: {SarimaTest_r2}\")\n",
    "#     print(f\"RMSE for SARIMA data: {SarimaTest_rmse}\")\n",
    "\n",
    "    # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (8, 6)\n",
    "\n",
    "    # Plot der Vorhersagen, Trainingsdaten und Testdaten\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(train_copy['date'], train_copy['value'], label='Trainingsdaten', color='blue')\n",
    "    plt.plot(test_copy['date'], test_copy['value'], label='Testdaten', color='green')\n",
    "    plt.plot(test_copy['date'], fc, color='red', linestyle='--', label='Vorhersagen (Test)')\n",
    "    plt.xlabel('Datum', fontsize=fontsize)\n",
    "    plt.ylabel('Wert', fontsize=fontsize)\n",
    "#     plt.title('SARIMA Vorhersagen vs. Trainings- und Testdaten', fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize_legende)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Exportiere das Diagramm als .png\n",
    "    plt.savefig('sarima_train.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    order=(p, d, q)\n",
    "    seasonal_order=(seasonal_p, seasonal_d, seasonal_q, periods)\n",
    "    \n",
    "    return (fc, SarimaTest_mae, SarimaTest_mse, SarimaTest_mape, SarimaTest_r2, SarimaTest_rmse, order, seasonal_order)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d59ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarima mit Fehlern im Testbereich\n",
    "# Versuch Sarima selbst zu optimieren mit Fehlermetriken\n",
    "\n",
    "\n",
    "def SARIMA100(train_data, test_data, freq_forecast, fehler, order, seasonal_order):\n",
    "    \n",
    "    # Arbeitskopie erstellen\n",
    "    train_copy = train_data.copy()\n",
    "    test_copy = test_data.copy()\n",
    "\n",
    "\n",
    "    if freq_forecast == 'D':\n",
    "        periods = 365\n",
    "    elif freq_forecast == 'W':\n",
    "        periods = 52\n",
    "    elif freq_forecast == 'M':\n",
    "        periods = 12\n",
    "    elif freq_forecast == 'Q':\n",
    "        periods = 4\n",
    "\n",
    "\n",
    "    # Erstelle und trainiere das ARIMA-Modell mit den extrahierten Parametern\n",
    "    model = statsmodels.tsa.arima.model.ARIMA(train_copy['value'],\n",
    "                                   order=order,\n",
    "                                   seasonal_order= seasonal_order,\n",
    "                                   enforce_stationarity=False,\n",
    "                                   enforce_invertibility=False)\n",
    "\n",
    "    results = model.fit()\n",
    "\n",
    "    \n",
    "\n",
    "    # Modellbewertung mit Testdaten\n",
    "    fc = results.forecast(steps=len(test_copy))\n",
    "\n",
    "    print('Prognose Mit SARIMA:')\n",
    "    print(fc)\n",
    "    # Modellbewertung mit Trainingsdaten\n",
    "    fc_train = results.forecast(steps=len(train_copy))\n",
    "\n",
    "    # Berechnen der Fehlermetriken für Testdaten\n",
    "    SarimaTest_mae = mean_absolute_error(test_copy['value'], fc)\n",
    "    SarimaTest_mse = mean_squared_error(test_copy['value'], fc)\n",
    "    SarimaTest_mape = mean_absolute_percentage_error(test_copy['value'], fc)\n",
    "    SarimaTest_r2 = r2_score(test_copy['value'], fc)\n",
    "    SarimaTest_rmse =  np.sqrt(mean_squared_error(test_copy['value'], fc))\n",
    "\n",
    "#     print(f\"Mean Absolute Error (MAE) for SARIMA data: {SarimaTest_mae}\")\n",
    "#     print(f\"Mean Squared Error (MSE) for SARIMA data: {SarimaTest_mse}\")\n",
    "#     print(f\"Mean Absolute Percentage Error (MAPE) for SARIMA data: {SarimaTest_mape}\")\n",
    "#     print(f\"R-squared (R^2) for SARIMA data: {SarimaTest_r2}\")\n",
    "#     print(f\"RMSE for SARIMA data: {SarimaTest_rmse}\")\n",
    "\n",
    "    # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (8, 6)\n",
    "\n",
    "    # Plot der Vorhersagen, Trainingsdaten und Testdaten\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(train_copy['date'], train_copy['value'], label='Trainingsdaten', color='blue')\n",
    "    plt.plot(test_copy['date'], test_copy['value'], label='Testdaten', color='green')\n",
    "    plt.plot(test_copy['date'], fc, color='red', linestyle='--', label='Vorhersagen (Test)')\n",
    "    plt.xlabel('Datum', fontsize=fontsize)\n",
    "    plt.ylabel('Wert', fontsize=fontsize)\n",
    "#     plt.title('SARIMA Vorhersagen vs. Trainings- und Testdaten', fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize_legende)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Exportiere das Diagramm als .png\n",
    "    plt.savefig('sarima_predictions.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return (fc, SarimaTest_mae, SarimaTest_mse, SarimaTest_mape, SarimaTest_r2, SarimaTest_rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd13b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet univariat mit Testfehlern\n",
    "\n",
    "def prophet80uni(train_data, test_data, freq):\n",
    "    # Prophet\n",
    "\n",
    "    # Arbeitskopien erstellen\n",
    "    train_copy = train_data.copy()\n",
    "    test_copy = test_data.copy()\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    df['ds'] = pd.to_datetime(train_copy['date'])\n",
    "    df['y'] = train_copy['value']\n",
    "\n",
    "    # Prophet-Modell mit Trainingsdaten erstellen und trainieren\n",
    "    prophet = Prophet()\n",
    "    prophet.fit(df)\n",
    "\n",
    "    future = prophet.make_future_dataframe(periods=len(test_copy), freq=freq)\n",
    "    forecast = prophet.predict(future)\n",
    "#     forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head()\n",
    "\n",
    "    # Setze negative Prognosewerte auf 0\n",
    "    forecast.loc[forecast['yhat'] < 0, 'yhat'] = 0\n",
    "    predicted_values_test = forecast.iloc[-len(test_copy):]['yhat']\n",
    "\n",
    "    # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (8, 6)\n",
    "\n",
    "    # Daten und Prognose plotten, mit Trainingsdaten, Testdaten, Prognose und tatsächlichen Daten\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot(train_copy['date'], train_copy['value'], label='Trainingsdaten', color='blue')  # Linie für Trainingsdaten hinzufügen\n",
    "    ax.plot(test_copy['date'], predicted_values_test, label='Prognose', color='red', linestyle='--')\n",
    "    # ax.scatter(test_copy['date'], test_copy['value'], color='r', label='Testdaten')\n",
    "    ax.plot(test_copy['date'], test_copy['value'], label='Tatsächliche Daten', color='green')  # Linie für die tatsächlichen Daten hinzufügen\n",
    "    ax.legend(fontsize=fontsize_legende)\n",
    "#     ax.set_title(\"Prophet Prognose für Testdaten\", fontsize=fontsize)\n",
    "    ax.set_xlabel('Datum', fontsize=fontsize)\n",
    "    ax.set_ylabel('Wert', fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Setze den Dateinamen für das Speichern des Diagramms\n",
    "    filename = 'prophet_uni_train.png'\n",
    "    if os.path.exists(filename):\n",
    "        filename = 'prophet_uni_forecast.png'\n",
    "        print('Prognose mit Prophet univariat:')\n",
    "        print(forecast['yhat'])\n",
    "\n",
    "    # Plot und speichern des Diagramms\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Fehlermetriken berechnen\n",
    "    actual_values_test = test_copy['value']\n",
    "    predicted_values_test = forecast.iloc[-len(test_copy):]['yhat']\n",
    "\n",
    "    FBPuTest_mae = mean_absolute_error(actual_values_test, predicted_values_test)\n",
    "    FBPuTest_mse = mean_squared_error(actual_values_test, predicted_values_test)\n",
    "    FBPuTest_mape = mean_absolute_percentage_error(actual_values_test, predicted_values_test)\n",
    "    FBPuTest_r2 = r2_score(actual_values_test, predicted_values_test)\n",
    "    FBPuTest_rmse = np.sqrt(mean_squared_error(actual_values_test, predicted_values_test))\n",
    "    \n",
    "    # Fehlermetriken für den Testbereich ausgeben\n",
    "#     print(\"Testbereich:\")\n",
    "#     print(f\"Mean Absolute Error (MAE): {FBPuTest_mae}\")\n",
    "#     print(f\"Mean Squared Error (MSE): {FBPuTest_mse}\")\n",
    "#     print(f\"Mean Absolute Percentage Error (MAPE): {FBPuTest_mape}\")\n",
    "#     print(f\"R-squared (R^2): {FBPuTest_r2}\")\n",
    "#     print(f\"RMSE: {FBPuTest_rmse}\")\n",
    "    \n",
    "    return (predicted_values_test, FBPuTest_mae, FBPuTest_mse, FBPuTest_mape, FBPuTest_r2, FBPuTest_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "563b2b6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    \n",
    "    for i in range(len(X) - time_steps + 1):  # Änderung hier\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)        \n",
    "        ys.append(y.iloc[i + time_steps - 1])  # Änderung hier\n",
    "    \n",
    "    return np.array(Xs).astype('float32'), np.array(ys).astype('float32')\n",
    "\n",
    "\n",
    "def LSTM80 (train_korr, valid_korr, fehler):\n",
    "    # Arbeitskopien erstellen\n",
    "    train_copy = train_korr.copy()\n",
    "    valid_copy = valid_korr.copy()\n",
    "    \n",
    "    if 'year' in train_copy.columns and 'month' in train_copy.columns and 'day' in train_copy.columns:\n",
    "        # Löschen der Spalten 'year', 'month' und 'day' aus train_copy\n",
    "        train_copy = train_copy.drop(['year', 'month', 'day'], axis=1)\n",
    "\n",
    "    if 'year' in valid_copy.columns and 'month' in valid_copy.columns and 'day' in valid_copy.columns:\n",
    "        # Löschen der Spalten 'year', 'month' und 'day' aus valid_copy\n",
    "        valid_copy = valid_copy.drop(['year', 'month', 'day'], axis=1)\n",
    "\n",
    "\n",
    "    \n",
    "    # Trainings- und Validierungsdaten zusammenführen\n",
    "    df = pd.concat([train_copy, valid_copy], ignore_index=True)\n",
    "    df.set_index('date', inplace=True)\n",
    "\n",
    "    # Normalisierung der Daten\n",
    "    train_max = df.max()\n",
    "    train_min = df.min()\n",
    "    train = (df - train_min) / (train_max - train_min)\n",
    "\n",
    "    # Aufteilen der Daten in Trainings- und Validierungssets\n",
    "    size_train = len(train_copy)\n",
    "    df_train = train.iloc[:size_train]\n",
    "    df_valid = train.iloc[size_train:]\n",
    "\n",
    "    time_steps = 1\n",
    "\n",
    "    # Erstellen der Trainings- und Validierungsdatensätze\n",
    "    X_train, y_train = create_dataset(df_train, df_train['value'], time_steps)\n",
    "    X_valid, y_valid = create_dataset(df_valid, df_valid['value'], time_steps)\n",
    "\n",
    "    # LSTM-Modell erstellen\n",
    "    model_lstm = Sequential(name='lstm_vale3')\n",
    "    model_lstm.add(LSTM(units=len(df_train.columns), return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "    model_lstm.add(LSTM(units=10, return_sequences=True))\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "    model_lstm.add(LSTM(units=10, return_sequences=True))\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "    model_lstm.add(LSTM(units=10))\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "    model_lstm.add(Dense(units=1))\n",
    "    \n",
    "    # Auswahl des Verlustfunktion basierend auf der angegebenen Fehlermetrik\n",
    "    if fehler in ['MSE', 'RMSE']:\n",
    "        loss_function = 'mean_squared_error'\n",
    "    elif fehler in ['MAE', 'R2']:\n",
    "        loss_function = 'mean_absolute_error'\n",
    "    else:\n",
    "        loss_function = 'mean_absolute_percentage_error'\n",
    "\n",
    "    model_lstm.compile(loss=loss_function, optimizer='adam')\n",
    "    model_lstm.summary()\n",
    "\n",
    "    # Modelltraining\n",
    "    history = model_lstm.fit(X_train, y_train, epochs=100, batch_size=30, shuffle=False, validation_data=(X_valid, y_valid), verbose=0)\n",
    "\n",
    "    y_pred = model_lstm.predict(X_valid)\n",
    "    y_pred_train = model_lstm.predict(X_train)\n",
    "\n",
    "\n",
    "    # Rescale the data back to the original scale\n",
    "    y_valid = y_valid*(train_max[0] - train_min[0]) + train_min[0]\n",
    "    y_pred = y_pred*(train_max[0] - train_min[0]) + train_min[0]\n",
    "    y_train = y_train*(train_max[0] - train_min[0]) + train_min[0]\n",
    "    y_pred_train = y_pred_train*(train_max[0] - train_min[0]) + train_min[0]\n",
    "\n",
    "    # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (8, 6)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(df_train.index, y_train, label='Trainingsdaten', color='blue')\n",
    "    plt.plot(df_valid.index, y_valid, label='Validierungsdaten', color='green')\n",
    "    plt.plot(df_train.index, y_pred_train, color='orange', label='Vorhersagen (Train)')\n",
    "    plt.plot(df_valid.index, y_pred, color='red', linestyle='--', label='Vorhersagen (Validierung)')\n",
    "    # plt.title('LSTM Vorhersagen vs. Trainings- und Validierungsdaten', fontsize=fontsize)\n",
    "    plt.xlabel('Datum', fontsize=fontsize)\n",
    "    plt.ylabel('Wert', fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize_legende)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Setze den Dateinamen für das Speichern des Diagramms\n",
    "    filename = 'LSTM_train.png'\n",
    "    if os.path.exists(filename):\n",
    "        filename = 'LSTM_forecast.png'\n",
    "        print('Prognose mit LSTM:')\n",
    "        print(y_pred)\n",
    "\n",
    "    # Plot und speichern des Diagramms\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Berechnung der Fehlermetriken für den Testbereich\n",
    "    LSTMValid_mae = mean_absolute_error(y_valid, y_pred)\n",
    "    LSTMValid_mse = mean_squared_error(y_valid, y_pred)\n",
    "    LSTMValid_mape = mean_absolute_percentage_error(y_valid, y_pred)\n",
    "    LSTMValid_r2 = r2_score(y_valid, y_pred)\n",
    "    LSTMValid_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "    \n",
    "#     print(\"Validierungsmetriken:\")\n",
    "#     print(\"MAE (Mean Absolute Error):\", LSTMValid_mae)\n",
    "#     print(\"MSE (Mean Squared Error):\", LSTMValid_mse)\n",
    "#     print(\"MAPE (Mean Absolute Percentage Error):\", LSTMValid_mape)\n",
    "#     print(\"R2 (R-squared):\", LSTMValid_r2)\n",
    "#     print(\"RMSE: \", LSTMValid_rmse)\n",
    "    \n",
    "    return (y_pred, LSTMValid_mae, LSTMValid_mse, LSTMValid_mape, LSTMValid_r2, LSTMValid_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a66e37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF80 (train_korr, valid_korr, fehler):\n",
    "    df_train = train_korr.copy()\n",
    "    df_valid = valid_korr.copy()\n",
    "\n",
    "    df_train.set_index('date', inplace=True)\n",
    "    df_valid.set_index('date', inplace=True)\n",
    "\n",
    "    df_without_target = df_train.columns\n",
    "    df_without_target = df_without_target.drop('value')\n",
    "\n",
    "    X_train = df_train[df_without_target]\n",
    "    y_train = df_train['value']\n",
    "    X_valid = df_valid[df_without_target]\n",
    "    y_valid = df_valid['value']\n",
    "\n",
    "    # RandomForest params dict\n",
    "    rf_params_one = {}\n",
    "    if fehler == 'MAE':\n",
    "        rf_params_one['criterion'] = 'mae'\n",
    "    elif fehler == 'MAPE':\n",
    "        rf_params_one['criterion'] = 'mae'\n",
    "    elif fehler == 'R2':\n",
    "        rf_params_one['criterion'] = 'mae'\n",
    "    else:\n",
    "        rf_params_one['criterion'] = 'mse'\n",
    "        \n",
    "    rf_params_one['n_estimators'] = 10\n",
    "    rf_params_one['max_depth'] = 5\n",
    "    rf_params_one['max_features'] = None\n",
    "    rf_params_one['max_leaf_nodes'] = 15\n",
    "    rf_params_one['min_samples_leaf'] = 1\n",
    "    rf_params_one['random_state'] = 0\n",
    "    rf_params_one['n_jobs'] = -1 # run all process\n",
    "\n",
    "    model_rf_regressor = RandomForestRegressor(**rf_params_one)\n",
    "\n",
    "    model_rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_rf_regressor = model_rf_regressor.predict(X_valid)\n",
    "    y_pred_rf_regressor_train = model_rf_regressor.predict(X_train)\n",
    "\n",
    "    # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (8, 6)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(df_train.index, y_train, label='Trainingsdaten', color='blue')\n",
    "    plt.plot(df_valid.index, y_valid, label='Validierungsdaten', color='green')\n",
    "    plt.plot(df_train.index, y_pred_rf_regressor_train, color='orange', label='Vorhersagen (Train)')\n",
    "    plt.plot(df_valid.index, y_pred_rf_regressor, color='red', linestyle='--', label='Vorhersagen (Validierung)')\n",
    "#     plt.title('RF Vorhersagen vs. Trainings- und Validierungsdaten', fontsize=fontsize)\n",
    "    plt.xlabel('Datum', fontsize=fontsize)\n",
    "    plt.ylabel('Wert', fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize_legende)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Überprüfe und speichere das Diagramm mit angepasstem Dateinamen\n",
    "    filename = 'rf_train.png'\n",
    "    if os.path.exists(filename):\n",
    "        filename = 'rf_forecast.png'\n",
    "        print('Prognose mit Random Forest:')\n",
    "        print(y_pred_rf_regressor)\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Berechnung der Metriken für den Validierungsbereich\n",
    "    RFValid_mae = mean_absolute_error(y_valid, y_pred_rf_regressor)\n",
    "    RFValid_mse = mean_squared_error(y_valid, y_pred_rf_regressor)\n",
    "    RFValid_mape = mean_absolute_percentage_error(y_valid, y_pred_rf_regressor)\n",
    "    RFValid_r2 = r2_score(y_valid, y_pred_rf_regressor)\n",
    "    RFValid_rmse = np.sqrt(mean_squared_error(y_valid, y_pred_rf_regressor))\n",
    "\n",
    "    # Ausgabe der Feature Importance\n",
    "    feature_importance = model_rf_regressor.feature_importances_\n",
    "    sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "#     for i, idx in enumerate(sorted_idx):\n",
    "#         print(f\"Feature {i+1}: {X_train.columns[idx]} - Importance: {feature_importance[idx]}\")\n",
    "    \n",
    "    # Sortiere die Ausgaben nach der Feature-Wichtigkeit\n",
    "    sorted_importance = sorted(zip(feature_importance, X_train.columns), reverse=True)\n",
    "\n",
    "    # Extrahiere die sortierten Werte und Features\n",
    "    sorted_values = [imp[0] for imp in sorted_importance]\n",
    "    sorted_features = [imp[1] for imp in sorted_importance]\n",
    "\n",
    "    # Erstelle das Balkendiagramm\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(range(len(sorted_features)), sorted_values, align='center')\n",
    "    plt.yticks(range(len(sorted_features)), sorted_features)\n",
    "    plt.xlabel('Importance')\n",
    "#     plt.ylabel('Features')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.gca().invert_yaxis()  # Umkehrung der Reihenfolge, damit das wichtigste Feature oben steht\n",
    "\n",
    "    # Exportiere das Balkendiagramm als Bild\n",
    "    output_filename = 'feature_importance_train.png'\n",
    "    \n",
    "      \n",
    "    if os.path.exists(output_filename):\n",
    "        filename = 'feature_importance_test.png'\n",
    "\n",
    "    plt.savefig(output_filename)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_filename)\n",
    "    plt.show()\n",
    "\n",
    "#     print(\"MAE (Mean Absolute Error) für den Validierungsbereich:\", RFValid_mae)\n",
    "#     print(\"MSE (Mean Squared Error) für den Validierungsbereich:\", RFValid_mse)\n",
    "#     print(\"MAPE (Mean Absolute Percentage Error) für den Validierungsbereich:\", RFValid_mape)\n",
    "#     print(\"R2 (R-squared) für den Validierungsbereich:\", RFValid_r2)\n",
    "#     print(\"RMSE: \", RFValid_rmse)\n",
    "    return (y_pred_rf_regressor, RFValid_mae, RFValid_mse, RFValid_mape, RFValid_r2, RFValid_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "faf8ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM80 (train_korr, valid_korr):\n",
    "    df_train = train_korr.copy()\n",
    "    df_valid = valid_korr.copy()\n",
    "\n",
    "    df_train.set_index('date', inplace=True)\n",
    "    df_valid.set_index('date', inplace=True)\n",
    "\n",
    "    df_without_target = df_train.columns\n",
    "    df_without_target = df_without_target.drop('value')\n",
    "\n",
    "    X_train = df_train[df_without_target]\n",
    "    y_train = df_train['value']\n",
    "    X_valid = df_valid[df_without_target]\n",
    "    y_valid = df_valid['value']\n",
    "\n",
    "    model_svm_regressor = SVR(kernel='rbf', gamma=0.1)\n",
    "\n",
    "    model_svm_regressor.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_svm_regressor = model_svm_regressor.predict(X_valid)\n",
    "    y_pred_svm_regressor_train = model_svm_regressor.predict(X_train)\n",
    "\n",
    "    # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (10, 6)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(df_train.index, y_train, label='Trainingsdaten', color='blue')\n",
    "    plt.plot(df_valid.index, y_valid, label='Validierungsdaten', color='green')\n",
    "    plt.plot(df_train.index, y_pred_svm_regressor_train, color='orange', label='Vorhersagen (Train)')\n",
    "    plt.plot(df_valid.index, y_pred_svm_regressor, color='red', linestyle='--', label='Vorhersagen (Validierung)')\n",
    "#     plt.title('SVM Vorhersagen vs. Trainings- und Validierungsdaten', fontsize=fontsize)\n",
    "    plt.xlabel('Datum', fontsize=fontsize)\n",
    "    plt.ylabel('Wert', fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize_legende)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Überprüfe und speichere das Diagramm mit angepasstem Dateinamen\n",
    "    filename = 'svm_train.png'\n",
    "    if os.path.exists(filename):\n",
    "        filename = 'svm_forecast.png'\n",
    "        print('Prognose mit SVM:')\n",
    "        print(y_pred_svm_regressor)\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Berechnung der Metriken für den Validierungsbereich\n",
    "    SVMValid_mae = mean_absolute_error(y_valid, y_pred_svm_regressor)\n",
    "    SVMValid_mse = mean_squared_error(y_valid, y_pred_svm_regressor)\n",
    "    SVMValid_mape = mean_absolute_percentage_error(y_valid, y_pred_svm_regressor)\n",
    "    SVMValid_r2 = r2_score(y_valid, y_pred_svm_regressor)\n",
    "    SVMValid_rmse = np.sqrt(mean_squared_error(y_valid, y_pred_svm_regressor))\n",
    "\n",
    "#     print(\"MAE (Mean Absolute Error) für den Validierungsbereich:\", SVMValid_mae)\n",
    "#     print(\"MSE (Mean Squared Error) für den Validierungsbereich:\", SVMValid_mse)\n",
    "#     print(\"MAPE (Mean Absolute Percentage Error) für den Validierungsbereich:\", SVMValid_mape)\n",
    "#     print(\"R2 (R-squared) für den Validierungsbereich:\", SVMValid_r2)\n",
    "#     print(\"RMSE: \", SVMValid_rmse)\n",
    "    return (y_pred_svm_regressor, SVMValid_mae, SVMValid_mse, SVMValid_mape, SVMValid_r2, SVMValid_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef80e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet80multi(train_korr, valid_korr, selected_countries):\n",
    "    train_copy = train_korr.copy()\n",
    "    valid_copy = valid_korr.copy()\n",
    "\n",
    "    # Umbenennen von Spalten für Prophet\n",
    "    train_copy.rename(columns={\"value\": 'y', \"date\": 'ds'}, inplace=True)\n",
    "    valid_copy.rename(columns={\"value\": 'y', \"date\": 'ds'}, inplace=True)\n",
    "\n",
    "    # Feiertage \n",
    "    years_train = train_copy['ds'].dt.year.unique()\n",
    "    years_valid = valid_copy['ds'].dt.year.unique()\n",
    "\n",
    "    # Kombinieren Sie die eindeutigen Jahre aus train und valid\n",
    "    all_years = np.unique(np.concatenate((years_train, years_valid)))\n",
    "\n",
    "    # Initialisieren einer leeren Liste für die Feiertagsvariablen\n",
    "    country_holidays = []\n",
    "\n",
    "    # Iterieren Sie durch die ausgewählten Länderkürzel und fügen Sie die Feiertagsmodule hinzu\n",
    "    for country in selected_countries:\n",
    "        try:\n",
    "            # Feiertage für das ausgewählte Land und Jahr hinzufügen\n",
    "            country_holidays.append(getattr(holidays, country)(years=all_years))\n",
    "        except AttributeError:\n",
    "            print(f\"Feiertage für {country} sind nicht verfügbar oder nicht implementiert.\")\n",
    "\n",
    "    # Kombinieren Sie die Feiertagsvariablen\n",
    "    all_holidays = sum(country_holidays, holidays.HolidayBase())\n",
    "\n",
    "    data_holidays = pd.DataFrame.from_dict(all_holidays, orient=\"index\").reset_index()\n",
    "    data_holidays.columns = [\"ds\", \"holiday\"]\n",
    "\n",
    "    # Prophet-Modell mit Trainingsdaten erstellen und trainieren\n",
    "    prophet = Prophet(holidays=data_holidays)\n",
    "\n",
    "    # Prophet-Modell mit zusätzlichen Regressoren erstellen und trainieren\n",
    "    for col in train_copy.columns:\n",
    "        if col not in ['ds', 'y']:  # Überspringe die Datumsspalte und die Zielvariable\n",
    "            prophet.add_regressor(col, standardize=False)\n",
    "\n",
    "    prophet.fit(train_copy)\n",
    "\n",
    "    # DataFrame für Vorhersagen für den Validierungsbereich erstellen\n",
    "    future_valid = pd.DataFrame()\n",
    "\n",
    "    for col in valid_copy.columns:\n",
    "        if col not in ['y']:  # Überspringe die Datumsspalte und die Zielvariable\n",
    "            future_valid[col] = valid_copy[col].values\n",
    "\n",
    "    forecast_valid = prophet.predict(future_valid)\n",
    "    forecast_valid[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head()\n",
    "\n",
    "    # Setze negative Prognosewerte auf 0\n",
    "    forecast_valid.loc[forecast_valid['yhat'] < 0, 'yhat'] = 0\n",
    "\n",
    "\n",
    "    # Fehlermetriken berechnen\n",
    "    actual_values_valid = valid_copy['y']\n",
    "    predicted_values_valid = forecast_valid['yhat']\n",
    "\n",
    "    FBPmValid_mae = mean_absolute_error(actual_values_valid, predicted_values_valid)\n",
    "    FBPmValid_mse = mean_squared_error(actual_values_valid, predicted_values_valid)\n",
    "    FBPmValid_mape = mean_absolute_percentage_error(actual_values_valid, predicted_values_valid)\n",
    "    FBPmValid_r2 = r2_score(actual_values_valid, predicted_values_valid)\n",
    "    FBPmValid_rmse = np.sqrt(mean_squared_error(actual_values_valid, predicted_values_valid))\n",
    "\n",
    "    # Fehlermetriken für den Validierungsbereich ausgeben\n",
    "#     print(\"Validierungsbereich:\")\n",
    "#     print(f\"Mean Absolute Error (MAE): {FBPmValid_mae}\")\n",
    "#     print(f\"Mean Squared Error (MSE): {FBPmValid_mse}\")\n",
    "#     print(f\"Mean Absolute Percentage Error (MAPE): {FBPmValid_mape}\")\n",
    "#     print(f\"R-squared (R^2): {FBPmValid_r2}\")\n",
    "#     print(f\"RMSE: {FBPmValid_rmse}\")\n",
    "    \n",
    "    # Setze die Parameter gemäß den Vorgaben\n",
    "    fontsize = 12\n",
    "    fontsize_legende = 8\n",
    "    figsize = (10, 6)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot(train_copy['ds'], train_copy['y'], label='Trainingsdaten', color='blue')\n",
    "    ax.plot(valid_copy['ds'], forecast_valid['yhat'], label='Prognose (Test)', linestyle='--', color='red')\n",
    "    ax.plot(valid_copy['ds'], valid_copy['y'], label='Tatsächliche Daten', color='green')\n",
    "    ax.legend(fontsize=fontsize_legende)\n",
    "#     ax.set_title(\"Prophet Prognose für Trainings- und Testdaten\", fontsize=fontsize)\n",
    "    ax.set_xlabel('Datum', fontsize=fontsize)\n",
    "    ax.set_ylabel('Wert', fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Überprüfe und speichere das Diagramm mit angepasstem Dateinamen\n",
    "    filename = 'prophet_train_test.png'\n",
    "    if os.path.exists(filename):\n",
    "        filename = 'prophet_forecast_test.png'\n",
    "        print('Prognose mit Prophet multivariat:')\n",
    "        print(forecast_valid['yhat'])\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    return (forecast_valid['yhat'], FBPmValid_mae, FBPmValid_mse, FBPmValid_mape, FBPmValid_r2, FBPmValid_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "63c2134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fehlermetriken für die Validbereiche der Modelle vergleichen\n",
    "def ergebnis_valid(linRegValid_mae, linRegValid_mse, linRegValid_rmse, linRegValid_mape, linRegValid_r2,\n",
    "                 CrosValid_mae, CrosValid_mse, CrosValid_rmse, CrosValid_mape, CrosValid_r2,\n",
    "                 ArimaValid_mae, ArimaValid_mse, ArimaValid_rmse, ArimaValid_mape, ArimaValid_r2,\n",
    "                 SarimaValid_mae, SarimaValid_mse, SarimaValid_rmse, SarimaValid_mape, SarimaValid_r2,\n",
    "                 FBPuValid_mae, FBPuValid_mse, FBPuValid_rmse, FBPuValid_mape, FBPuValid_r2,\n",
    "                 LSTMValid_mae, LSTMValid_mse, LSTMValid_rmse, LSTMValid_mape, LSTMValid_r2,\n",
    "                 RFValid_mae, RFValid_mse, RFValid_rmse, RFValid_mape, RFValid_r2,\n",
    "                 SVMValid_mae, SVMValid_mse, SVMValid_rmse, SVMValid_mape, SVMValid_r2,\n",
    "                 FBPmValid_mae, FBPmValid_mse, FBPmValid_rmse, FBPmValid_mape, FBPmValid_r2,\n",
    "                 HWAddValid_mae, HWAddValid_mse, HWAddValid_rmse, HWAddValid_mape, HWAddValid_r2,\n",
    "                 HWMulValid_mae, HWMulValid_mse, HWMulValid_rmse, HWMulValid_mape, HWMulValid_r2, \n",
    "                 LSTM_b_Valid_mae, LSTM_b_Valid_mse, LSTM_b_Valid_rmse, LSTM_b_Valid_mape, LSTM_b_Valid_r2):\n",
    "\n",
    "\n",
    "    # Annahme: Sie haben bereits die Fehlermetriken für jeden Modelltyp im Validierungsbereich berechnet und in Variablen gespeichert\n",
    "\n",
    "    # Fehlermetriken für das linReg-Modell\n",
    "    linReg_metrics = [linRegValid_mae, linRegValid_mse, linRegValid_rmse, linRegValid_mape, linRegValid_r2]\n",
    "\n",
    "    # Fehlermetriken für das Cros-Modell\n",
    "    Cros_metrics = [CrosValid_mae, CrosValid_mse, CrosValid_rmse, CrosValid_mape, CrosValid_r2]\n",
    "\n",
    "    # Fehlermetriken für das Arima-Modell\n",
    "    Arima_metrics = [ArimaValid_mae, ArimaValid_mse, ArimaValid_rmse, ArimaValid_mape, ArimaValid_r2]\n",
    "\n",
    "    # Fehlermetriken für das FBPu-Modell\n",
    "    FBPu_metrics = [FBPuValid_mae, FBPuValid_mse, FBPuValid_rmse, FBPuValid_mape, FBPuValid_r2]\n",
    "\n",
    "    # Fehlermetriken für das Sarima-Modell\n",
    "    Sarima_metrics = [SarimaValid_mae, SarimaValid_mse, SarimaValid_rmse, SarimaValid_mape, SarimaValid_r2]\n",
    "\n",
    "    # Fehlermetriken für das LSTM-Modell\n",
    "    LSTM_metrics = [LSTMValid_mae, LSTMValid_mse, LSTMValid_rmse, LSTMValid_mape, LSTMValid_r2]\n",
    "\n",
    "    # Fehlermetriken für das RF-Modell\n",
    "    RF_metrics = [RFValid_mae, RFValid_mse, RFValid_rmse, RFValid_mape, RFValid_r2]\n",
    "\n",
    "    # Fehlermetriken für das SVM-Modell\n",
    "    SVM_metrics = [SVMValid_mae, SVMValid_mse, SVMValid_rmse, SVMValid_mape, SVMValid_r2]\n",
    "\n",
    "    # Fehlermetriken für das FBPm-Modell\n",
    "    FBPm_metrics = [FBPmValid_mae, FBPmValid_mse, FBPmValid_rmse, FBPmValid_mape, FBPmValid_r2]\n",
    "\n",
    "    # Fehlermetriken für das LSTM_b-Modell\n",
    "    LSTM_b_metrics = [LSTM_b_Valid_mae, LSTM_b_Valid_mse, LSTM_b_Valid_rmse, LSTM_b_Valid_mape, LSTM_b_Valid_r2]\n",
    "\n",
    "    # Fehlermetriken für das HWAdd-Modell (abhängig von 'marker')\n",
    "    if marker:\n",
    "        HWAdd_metrics = [HWAddValid_mae, HWAddValid_mse, HWAddValid_rmse, HWAddValid_mape, HWAddValid_r2]\n",
    "        HWMul_metrics = [HWMulValid_mae, HWMulValid_mse, HWMulValid_rmse, HWMulValid_mape, HWMulValid_r2]\n",
    "    else:\n",
    "        HWAdd_metrics = [HWAddValid_mae, HWAddValid_mse, HWAddValid_rmse, HWAddValid_mape, HWAddValid_r2]\n",
    "\n",
    "    # Liste der Modelle\n",
    "    if marker: \n",
    "        models = ['LinReg', 'Croston', 'ARIMA', 'Prophet-u', 'SARIMA', 'LSTM', 'RF', 'SVR', 'Prophet-m', 'H-W_add', 'H-W_mul', 'LSTM_BFS']\n",
    "    else:\n",
    "        models = ['LinReg', 'Croston', 'ARIMA', 'Prophet-u', 'SARIMA', 'LSTM', 'RF', 'SVR', 'Prophet-m', 'H-W_add', 'LSTM_BFS']\n",
    "\n",
    "    # Erstellen eines DataFrame für den Vergleich der Fehlermetriken im Validierungsbereich\n",
    "    if marker: \n",
    "        comparison_df_valid = pd.DataFrame({\n",
    "            'Model': models,\n",
    "            'MAE': [linReg_metrics[0], Cros_metrics[0], Arima_metrics[0], FBPu_metrics[0], Sarima_metrics[0], LSTM_metrics[0], RF_metrics[0], SVM_metrics[0], FBPm_metrics[0], HWAdd_metrics[0], HWMul_metrics[0], LSTM_b_metrics[0]],\n",
    "            'MSE': [linReg_metrics[1], Cros_metrics[1], Arima_metrics[1], FBPu_metrics[1], Sarima_metrics[1], LSTM_metrics[1], RF_metrics[1], SVM_metrics[1], FBPm_metrics[1], HWAdd_metrics[1], HWMul_metrics[1], LSTM_b_metrics[1]],\n",
    "            'RMSE': [linReg_metrics[2], Cros_metrics[2], Arima_metrics[2], FBPu_metrics[2], Sarima_metrics[2], LSTM_metrics[2], RF_metrics[2], SVM_metrics[2], FBPm_metrics[2], HWAdd_metrics[2], HWMul_metrics[2], LSTM_b_metrics[2]],\n",
    "            'MAPE': [linReg_metrics[3], Cros_metrics[3], Arima_metrics[3], FBPu_metrics[3], Sarima_metrics[3], LSTM_metrics[3], RF_metrics[3], SVM_metrics[3], FBPm_metrics[3], HWAdd_metrics[3], HWMul_metrics[3], LSTM_b_metrics[3]],\n",
    "            'R-squared': [linReg_metrics[4], Cros_metrics[4], Arima_metrics[4], FBPu_metrics[4], Sarima_metrics[4], LSTM_metrics[4], RF_metrics[4], SVM_metrics[4], FBPm_metrics[4], HWAdd_metrics[4], HWMul_metrics[4], LSTM_b_metrics[4]]\n",
    "        })\n",
    "    else:\n",
    "        comparison_df_valid = pd.DataFrame({\n",
    "            'Model': models,\n",
    "            'MAE': [linReg_metrics[0], Cros_metrics[0], Arima_metrics[0], FBPu_metrics[0], Sarima_metrics[0], LSTM_metrics[0], RF_metrics[0], SVM_metrics[0], FBPm_metrics[0], HWAdd_metrics[0], LSTM_b_metrics[0]],\n",
    "            'MSE': [linReg_metrics[1], Cros_metrics[1], Arima_metrics[1], FBPu_metrics[1], Sarima_metrics[1], LSTM_metrics[1], RF_metrics[1], SVM_metrics[1], FBPm_metrics[1], HWAdd_metrics[1], LSTM_b_metrics[1]],\n",
    "            'RMSE': [linReg_metrics[2], Cros_metrics[2], Arima_metrics[2], FBPu_metrics[2], Sarima_metrics[2], LSTM_metrics[2], RF_metrics[2], SVM_metrics[2], FBPm_metrics[2], HWAdd_metrics[2], LSTM_b_metrics[2]],\n",
    "            'MAPE': [linReg_metrics[3], Cros_metrics[3], Arima_metrics[3], FBPu_metrics[3], Sarima_metrics[3], LSTM_metrics[3], RF_metrics[3], SVM_metrics[3], FBPm_metrics[3], HWAdd_metrics[3], LSTM_b_metrics[3]],\n",
    "            'R-squared': [linReg_metrics[4], Cros_metrics[4], Arima_metrics[4], FBPu_metrics[4], Sarima_metrics[4], LSTM_metrics[4], RF_metrics[4], SVM_metrics[4], FBPm_metrics[4], HWAdd_metrics[4], LSTM_b_metrics[4]]\n",
    "        })\n",
    "    print(comparison_df_valid)\n",
    "\n",
    "    # # Überprüfen, ob die Variablen bereits definiert sind, bevor sie gelöscht werden\n",
    "    # if 'top_mae_models' in globals():\n",
    "    #     del top_mae_models\n",
    "    # if 'top_mse_models' in globals():\n",
    "    #     del top_mse_models\n",
    "    # if 'top_rmse_models' in globals():\n",
    "    #     del top_rmse_models\n",
    "    # if 'top_mape_models' in globals():\n",
    "    #     del top_mape_models\n",
    "    # if 'top_r2_models' in globals():\n",
    "    #     del top_r2_models\n",
    "\n",
    "\n",
    "    # Für MAE, MSE, RMSE und MAPE die 3 Modelle mit dem geringsten Wert finden\n",
    "    top_mae_models = comparison_df_valid.nsmallest(3, 'MAE')\n",
    "    top_mse_models = comparison_df_valid.nsmallest(3, 'MSE')\n",
    "    top_rmse_models = comparison_df_valid.nsmallest(3, 'RMSE')\n",
    "    top_mape_models = comparison_df_valid.nsmallest(3, 'MAPE')\n",
    "\n",
    "    # Für R-squared die 3 Modelle mit dem größten Wert finden\n",
    "    top_r2_models = comparison_df_valid.nlargest(3, 'R-squared')\n",
    "\n",
    "    # Erstellen des Säulendiagramms für jede Fehlermetrik\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(8, 12))\n",
    "\n",
    "    # MAE\n",
    "    ax = axs[0, 0]\n",
    "    bars = ax.bar(comparison_df_valid['Model'], comparison_df_valid['MAE'], color='skyblue')\n",
    "    ax.set_title('a)', loc='left')\n",
    "    ax.set_ylabel('MAE')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    # Werte der Balken in 45 Grad über den Balken anzeigen\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        text_height = y_min + (y_max - y_min) * 0.05  # 5% der Höhe über der minimalen Y-Achsenhöhe\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, text_height, f'{height:.2f}', ha='center', va='bottom', rotation=90)\n",
    "\n",
    "    # MSE\n",
    "    ax = axs[0, 1]\n",
    "    bars = ax.bar(comparison_df_valid['Model'], comparison_df_valid['MSE'], color='lightgreen')\n",
    "    ax.set_title('b)', loc='left')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    # Werte der Balken in 45 Grad über den Balken anzeigen\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        text_height = y_min + (y_max - y_min) * 0.05  # 5% der Höhe über der minimalen Y-Achsenhöhe\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, text_height, f'{height:.2f}', ha='center', va='bottom', rotation=90)\n",
    "\n",
    "    # RMSE\n",
    "    ax = axs[1, 0]\n",
    "    bars = ax.bar(comparison_df_valid['Model'], comparison_df_valid['RMSE'], color='lightcoral')\n",
    "    ax.set_title('c)', loc='left')\n",
    "    ax.set_ylabel('RMSE')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    # Werte der Balken in 45 Grad über den Balken anzeigen\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        text_height = y_min + (y_max - y_min) * 0.05  # 5% der Höhe über der minimalen Y-Achsenhöhe\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, text_height, f'{height:.2f}', ha='center', va='bottom', rotation=90)\n",
    "\n",
    "    # MAPE\n",
    "    ax = axs[1, 1]\n",
    "    bars = ax.bar(comparison_df_valid['Model'], comparison_df_valid['MAPE'], color='salmon')\n",
    "    ax.set_title('d)', loc='left')\n",
    "    ax.set_ylabel('MAPE')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    # Werte der Balken in 45 Grad über den Balken anzeigen\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        text_height = y_min + (y_max - y_min) * 0.05  # 5% der Höhe über der minimalen Y-Achsenhöhe\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, text_height, f'{height:.2f}', ha='center', va='bottom', rotation=90)\n",
    "\n",
    "    # R-squared\n",
    "    ax = axs[2, 0]\n",
    "    bars = ax.bar(comparison_df_valid['Model'], comparison_df_valid['R-squared'], color='lightcoral')\n",
    "    ax.set_title('e)', loc='left')\n",
    "    ax.set_ylabel('R-squared')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    # Werte der Balken in 45 Grad über den Balken anzeigen\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        text_height = y_min + (y_max - y_min) * 0.8  # 5% der Höhe über der minimalen Y-Achsenhöhe\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, text_height, f'{height:.2f}', ha='center', va='bottom', rotation=90)\n",
    "\n",
    "#     # Textfeld für die besten Modelle für jede Fehlermetrik\n",
    "#     for ax, top_models, metric_name in zip(axs.flat, [top_mae_models, top_mse_models, top_rmse_models, top_mape_models, top_r2_models], ['MAE', 'MSE', 'RMSE', 'MAPE', 'R-squared']):\n",
    "#         best_models_names = top_models['Model'].tolist()\n",
    "#         best_models_values = top_models[metric_name].tolist()\n",
    "#         ax.text(0.95, 0.85, f'Best Models for {metric_name}:\\n1. {best_models_names[0]}: {best_models_values[0]:.3f}\\n2. {best_models_names[1]}: {best_models_values[1]:.3f}\\n3. {best_models_names[2]}: {best_models_values[2]:.3f}',\n",
    "#                 transform=ax.transAxes, fontsize=10, verticalalignment='top', horizontalalignment='right', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Exportieren des Plots als PNG-Datei\n",
    "    plt.savefig('Fehlerübersicht Train.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Printen der Übersicht\n",
    "    for top_models, metric_name in zip([top_mae_models, top_mse_models, top_rmse_models, top_mape_models, top_r2_models], ['MAE', 'MSE', 'RMSE', 'MAPE', 'R-squared']):\n",
    "        best_models_names = top_models['Model'].tolist()\n",
    "        best_models_values = top_models[metric_name].tolist()\n",
    "        print(f'Best Models for {metric_name}:')\n",
    "        for i, (model_name, metric_value) in enumerate(zip(best_models_names, best_models_values), start=1):\n",
    "            print(f'{i}. {model_name}: {metric_value:.3f}')\n",
    "        print()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9cc2466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fehlermetriken für die Testbereiche der Modelle vergleichen\n",
    "def ergebnis_test(linRegTest_mae, linRegTest_mse, linRegTest_rmse, linRegTest_mape, linRegTest_r2,\n",
    "                 CrosTest_mae, CrosTest_mse, CrosTest_rmse, CrosTest_mape, CrosTest_r2,\n",
    "                 ArimaTest_mae, ArimaTest_mse, ArimaTest_rmse, ArimaTest_mape, ArimaTest_r2,\n",
    "                 SarimaTest_mae, SarimaTest_mse, SarimaTest_rmse, SarimaTest_mape, SarimaTest_r2,\n",
    "                 FBPuTest_mae, FBPuTest_mse, FBPuTest_rmse, FBPuTest_mape, FBPuTest_r2,\n",
    "                 LSTMTest_mae, LSTMTest_mse, LSTMTest_rmse, LSTMTest_mape, LSTMTest_r2,\n",
    "                 RFTest_mae, RFTest_mse, RFTest_rmse, RFTest_mape, RFTest_r2,\n",
    "                 SVMTest_mae, SVMTest_mse, SVMTest_rmse, SVMTest_mape, SVMTest_r2,\n",
    "                 FBPmTest_mae, FBPmTest_mse, FBPmTest_rmse, FBPmTest_mape, FBPmTest_r2,\n",
    "                 HWAddTest_mae, HWAddTest_mse, HWAddTest_rmse, HWAddTest_mape, HWAddTest_r2,\n",
    "                 HWMulTest_mae, HWMulTest_mse, HWMulTest_rmse, HWMulTest_mape, HWMulTest_r2,\n",
    "                 LSTM_b_Test_mae, LSTM_b_Test_mse, LSTM_b_Test_rmse, LSTM_b_Test_mape, LSTM_b_Test_r2):\n",
    "    # Fehlermetriken für das linReg-Modell\n",
    "    linReg_metrics = [linRegTest_mae, linRegTest_mse, linRegTest_rmse, linRegTest_mape, linRegTest_r2]\n",
    "\n",
    "    # Fehlermetriken für das Cros-Modell\n",
    "    Cros_metrics = [CrosTest_mae, CrosTest_mse, CrosTest_rmse, CrosTest_mape, CrosTest_r2]\n",
    "\n",
    "    # Fehlermetriken für das Arima-Modell\n",
    "    Arima_metrics = [ArimaTest_mae, ArimaTest_mse, ArimaTest_rmse, ArimaTest_mape, ArimaTest_r2]\n",
    "\n",
    "    # Fehlermetriken für das FBPu-Modell\n",
    "    FBPu_metrics = [FBPuTest_mae, FBPuTest_mse, FBPuTest_rmse, FBPuTest_mape, FBPuTest_r2]\n",
    "\n",
    "    # Fehlermetriken für das Sarima-Modell\n",
    "    Sarima_metrics = [SarimaTest_mae, SarimaTest_mse, SarimaTest_rmse, SarimaTest_mape, SarimaTest_r2]\n",
    "\n",
    "    # Fehlermetriken für das LSTM-Modell\n",
    "    LSTM_metrics = [LSTMTest_mae, LSTMTest_mse, LSTMTest_rmse, LSTMTest_mape, LSTMTest_r2]\n",
    "\n",
    "    # Fehlermetriken für das RF-Modell\n",
    "    RF_metrics = [RFTest_mae, RFTest_mse, RFTest_rmse, RFTest_mape, RFTest_r2]\n",
    "\n",
    "    # Fehlermetriken für das SVM-Modell\n",
    "    SVM_metrics = [SVMTest_mae, SVMTest_mse, SVMTest_rmse, SVMTest_mape, SVMTest_r2]\n",
    "\n",
    "    # Fehlermetriken für das FBPm-Modell\n",
    "    FBPm_metrics = [FBPmTest_mae, FBPmTest_mse, FBPmTest_rmse, FBPmTest_mape, FBPmTest_r2]\n",
    "\n",
    "    # Fehlermetriken für das LSTM_b-Modell\n",
    "    LSTM_b_metrics = [LSTM_b_Test_mae, LSTM_b_Test_mse, LSTM_b_Test_rmse, LSTM_b_Test_mape, LSTM_b_Test_r2]\n",
    "\n",
    "    # Fehlermetriken für das HWAdd-Modell (abhängig von 'marker')\n",
    "    if marker:\n",
    "        HWAdd_metrics = [HWAddTest_mae, HWAddTest_mse, HWAddTest_rmse, HWAddTest_mape, HWAddTest_r2]\n",
    "        HWMul_metrics = [HWMulTest_mae, HWMulTest_mse, HWMulTest_rmse, HWMulTest_mape, HWMulTest_r2]\n",
    "    else:\n",
    "        HWAdd_metrics = [HWAddTest_mae, HWAddTest_mse, HWAddTest_rmse, HWAddTest_mape, HWAddTest_r2]\n",
    "\n",
    "    # Liste der Modelle\n",
    "    if marker: \n",
    "        models = ['LinReg', 'Croston', 'ARIMA', 'Prophet-u', 'SARIMA', 'LSTM', 'RF', 'SVR', 'Prophet-m', 'H-W_add', 'H-W_mul', 'LSTM_BFS']\n",
    "    else:\n",
    "        models = ['LinReg', 'Croston', 'ARIMA', 'Prophet-u', 'SARIMA', 'LSTM', 'RF', 'SVR', 'Prophet-m', 'H-W_add', 'LSTM_BFS']\n",
    "\n",
    "    # Erstellen eines DataFrame für den Vergleich der Fehlermetriken im Validierungsbereich\n",
    "    if marker: \n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Model': models,\n",
    "            'MAE': [linReg_metrics[0], Cros_metrics[0], Arima_metrics[0], FBPu_metrics[0], Sarima_metrics[0], LSTM_metrics[0], RF_metrics[0], SVM_metrics[0], FBPm_metrics[0], HWAdd_metrics[0], HWMul_metrics[0], LSTM_b_metrics[0]],\n",
    "            'MSE': [linReg_metrics[1], Cros_metrics[1], Arima_metrics[1], FBPu_metrics[1], Sarima_metrics[1], LSTM_metrics[1], RF_metrics[1], SVM_metrics[1], FBPm_metrics[1], HWAdd_metrics[1], HWMul_metrics[1], LSTM_b_metrics[1]],\n",
    "            'RMSE': [linReg_metrics[2], Cros_metrics[2], Arima_metrics[2], FBPu_metrics[2], Sarima_metrics[2], LSTM_metrics[2], RF_metrics[2], SVM_metrics[2], FBPm_metrics[2], HWAdd_metrics[2], HWMul_metrics[2], LSTM_b_metrics[2]],\n",
    "            'MAPE': [linReg_metrics[3], Cros_metrics[3], Arima_metrics[3], FBPu_metrics[3], Sarima_metrics[3], LSTM_metrics[3], RF_metrics[3], SVM_metrics[3], FBPm_metrics[3], HWAdd_metrics[3], HWMul_metrics[3], LSTM_b_metrics[3]],\n",
    "            'R-squared': [linReg_metrics[4], Cros_metrics[4], Arima_metrics[4], FBPu_metrics[4], Sarima_metrics[4], LSTM_metrics[4], RF_metrics[4], SVM_metrics[4], FBPm_metrics[4], HWAdd_metrics[4], HWMul_metrics[4], LSTM_b_metrics[4]]\n",
    "        })\n",
    "    else:\n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Model': models,\n",
    "            'MAE': [linReg_metrics[0], Cros_metrics[0], Arima_metrics[0], FBPu_metrics[0], Sarima_metrics[0], LSTM_metrics[0], RF_metrics[0], SVM_metrics[0], FBPm_metrics[0], HWAdd_metrics[0], LSTM_b_metrics[0]],\n",
    "            'MSE': [linReg_metrics[1], Cros_metrics[1], Arima_metrics[1], FBPu_metrics[1], Sarima_metrics[1], LSTM_metrics[1], RF_metrics[1], SVM_metrics[1], FBPm_metrics[1], HWAdd_metrics[1], LSTM_b_metrics[1]],\n",
    "            'RMSE': [linReg_metrics[2], Cros_metrics[2], Arima_metrics[2], FBPu_metrics[2], Sarima_metrics[2], LSTM_metrics[2], RF_metrics[2], SVM_metrics[2], FBPm_metrics[2], HWAdd_metrics[2], LSTM_b_metrics[2]],\n",
    "            'MAPE': [linReg_metrics[3], Cros_metrics[3], Arima_metrics[3], FBPu_metrics[3], Sarima_metrics[3], LSTM_metrics[3], RF_metrics[3], SVM_metrics[3], FBPm_metrics[3], HWAdd_metrics[3], LSTM_b_metrics[3]],\n",
    "            'R-squared': [linReg_metrics[4], Cros_metrics[4], Arima_metrics[4], FBPu_metrics[4], Sarima_metrics[4], LSTM_metrics[4], RF_metrics[4], SVM_metrics[4], FBPm_metrics[4], HWAdd_metrics[4], LSTM_b_metrics[4]]\n",
    "        })\n",
    "\n",
    "    print(comparison_df)\n",
    "\n",
    "    #     # Überprüfen, ob die Variablen bereits definiert sind, bevor sie gelöscht werden\n",
    "    #     if 'top_mae_models' in globals():\n",
    "    #         del top_mae_models\n",
    "    #     if 'top_mse_models' in globals():\n",
    "    #         del top_mse_models\n",
    "    #     if 'top_rmse_models' in globals():\n",
    "    #         del top_rmse_models\n",
    "    #     if 'top_mape_models' in globals():\n",
    "    #         del top_mape_models\n",
    "    #     if 'top_r2_models' in globals():\n",
    "    #         del top_r2_models\n",
    "\n",
    "    # Für MAE, MSE, RMSE und MAPE die 3 Modelle mit dem geringsten Wert finden\n",
    "    top_mae_models = comparison_df.nsmallest(3, 'MAE')\n",
    "    top_mse_models = comparison_df.nsmallest(3, 'MSE')\n",
    "    top_rmse_models = comparison_df.nsmallest(3, 'RMSE')\n",
    "    top_mape_models = comparison_df.nsmallest(3, 'MAPE')\n",
    "\n",
    "    # Für R-squared die 3 Modelle mit dem größten Wert finden\n",
    "    top_r2_models = comparison_df.nlargest(3, 'R-squared')\n",
    "\n",
    "    # Erstellen des Säulendiagramms für jede Fehlermetrik\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(8, 12))\n",
    "\n",
    "    # MAE\n",
    "    ax = axs[0, 0]\n",
    "    bars = ax.bar(comparison_df['Model'], comparison_df['MAE'], color='skyblue')\n",
    "    ax.set_title('a)', loc='left')\n",
    "    ax.set_ylabel('MAE')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    # Werte der Balken in 90 Grad unterhalb der x-Achse anzeigen\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        text_height = y_min + (y_max - y_min) * 0.05  # 5% der Höhe über der minimalen Y-Achsenhöhe\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, text_height, f'{height:.2f}', ha='center', va='bottom', rotation=90)\n",
    "\n",
    "    # MSE\n",
    "    ax = axs[0, 1]\n",
    "    bars = ax.bar(comparison_df['Model'], comparison_df['MSE'], color='lightgreen')\n",
    "    ax.set_title('b)', loc='left')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    # Werte der Balken in 90 Grad unterhalb der x-Achse anzeigen\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        text_height = y_min + (y_max - y_min) * 0.05  # 5% der Höhe über der minimalen Y-Achsenhöhe\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, text_height, f'{height:.2f}', ha='center', va='bottom', rotation=90)\n",
    "\n",
    "    # RMSE\n",
    "    ax = axs[1, 0]\n",
    "    bars = ax.bar(comparison_df['Model'], comparison_df['RMSE'], color='lightcoral')\n",
    "    ax.set_title('c)', loc='left')\n",
    "    ax.set_ylabel('RMSE')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    # Werte der Balken in 90 Grad unterhalb der x-Achse anzeigen\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        text_height = y_min + (y_max - y_min) * 0.05  # 5% der Höhe über der minimalen Y-Achsenhöhe\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, text_height, f'{height:.2f}', ha='center', va='bottom', rotation=90)\n",
    "\n",
    "    # MAPE\n",
    "    ax = axs[1, 1]\n",
    "    bars = ax.bar(comparison_df['Model'], comparison_df['MAPE'], color='salmon')\n",
    "    ax.set_title('d)', loc='left')\n",
    "    ax.set_ylabel('MAPE')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    # Werte der Balken in 90 Grad unterhalb der x-Achse anzeigen\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        text_height = y_min + (y_max - y_min) * 0.05  # 5% der Höhe über der minimalen Y-Achsenhöhe\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, text_height, f'{height:.2f}', ha='center', va='bottom', rotation=90)\n",
    "\n",
    "    # R-squared\n",
    "    ax = axs[2, 0]\n",
    "    bars = ax.bar(comparison_df['Model'], comparison_df['R-squared'], color='lightcoral')\n",
    "    ax.set_title('e)', loc='left')\n",
    "    ax.set_ylabel('R-squared')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    # Werte der Balken in 90 Grad unterhalb der x-Achse anzeigen\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        text_height = y_min + (y_max - y_min) * 0.8  # 5% der Höhe über der minimalen Y-Achsenhöhe\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, text_height, f'{height:.2f}', ha='center', va='bottom', rotation=90)\n",
    "\n",
    "    # Anzeigen der Plots\n",
    "    plt.tight_layout()\n",
    "    # Exportieren des Plots als PNG-Datei\n",
    "    plt.savefig('Fehlerübersicht Test.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    #     # Textfeld für die besten Modelle für jede Fehlermetrik\n",
    "    #     for ax, top_models, metric_name in zip(axs.flat, [top_mae_models, top_mse_models, top_rmse_models, top_mape_models, top_r2_models], ['MAE', 'MSE', 'RMSE', 'MAPE', 'R-squared']):\n",
    "    #         best_models_names = top_models['Model'].tolist()\n",
    "    #         best_models_values = top_models[metric_name].tolist()\n",
    "    #         ax.text(0.95, 0.85, f'Best Models for {metric_name}:\\n1. {best_models_names[0]}: {best_models_values[0]:.3f}\\n2. {best_models_names[1]}: {best_models_values[1]:.3f}\\n3. {best_models_names[2]}: {best_models_values[2]:.3f}',\n",
    "    #                 transform=ax.transAxes, fontsize=10, verticalalignment='top', horizontalalignment='right', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "\n",
    "    # Anzeigen der Plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Printen der Übersicht\n",
    "    for top_models, metric_name in zip([top_mae_models, top_mse_models, top_rmse_models, top_mape_models, top_r2_models], ['MAE', 'MSE', 'RMSE', 'MAPE', 'R-squared']):\n",
    "        best_models_names = top_models['Model'].tolist()\n",
    "        best_models_values = top_models[metric_name].tolist()\n",
    "        print(f'Best Models for {metric_name}:')\n",
    "        for i, (model_name, metric_value) in enumerate(zip(best_models_names, best_models_values), start=1):\n",
    "            print(f'{i}. {model_name}: {metric_value:.3f}')\n",
    "        print()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6d2b0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Umwandlung von Cros_4 und LinReg_4 in Pandas-Datenrahmen mit dem Index von Arima_4\n",
    "Cros_4_df = pd.DataFrame({'Cros_4': Cros_4}, index=Arima_4.index)\n",
    "LinReg_4_df = pd.DataFrame({'LinReg_4': linReg_4}, index=Arima_4.index)\n",
    "\n",
    "# Setzen der Schriftgröße\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Setzen der Größe des Plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot für LinReg_4 (mit Arima-Index)\n",
    "plt.plot(valid_data['date'], LinReg_4_df, label='LinReg')\n",
    "\n",
    "# Plot für Cros_4 (mit Arima-Index)\n",
    "plt.plot(valid_data['date'], Cros_4_df, label='Croston')\n",
    "\n",
    "# Plot für Arima_4\n",
    "plt.plot(valid_data['date'], Arima_4, label='ARIMA')\n",
    "\n",
    "# Plot für Sarima_4\n",
    "plt.plot(valid_data['date'], Sarima_4, label='SARIMA')\n",
    "\n",
    "# Plot für valid_data\n",
    "plt.plot(valid_data['date'], valid_data['value'], label='Validierungsdaten', color='darkgreen', linestyle='--')\n",
    "\n",
    "# Plot für Trainingsdaten\n",
    "plt.plot(train_data['date'], train_data['value'], label='Trainingsdaten', color='black', linestyle='--')\n",
    "\n",
    "# Legende anzeigen außerhalb des Plots (rechts)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Absatz [Stück]')\n",
    "\n",
    "# Gitter anzeigen\n",
    "plt.grid(True)\n",
    "\n",
    "# Drehung der x-Achsenbeschriftungen um 45 Grad\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.savefig('LinReg, Cros, Arima, Satima_train_kom.png', bbox_inches='tight')  # Exportieren als PNG\n",
    "# Plot anzeigen\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Umwandlung von Cros_4 und LinReg_4 in Pandas-Datenrahmen mit dem Index von Arima_4\n",
    "Cros_4_df = pd.DataFrame({'Cros_4': Cros_4}, index=Arima_4.index)\n",
    "LinReg_4_df = pd.DataFrame({'LinReg_4': linReg_4}, index=Arima_4.index)\n",
    "\n",
    "# Setzen der Schriftgröße\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Setzen der Größe des Plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot für LinReg_4 (mit Arima-Index)\n",
    "plt.plot(valid_data['date'], LinReg_4_df, label='LinReg', linestyle='-.', color='blue')\n",
    "\n",
    "# Plot für Cros_4 (mit Arima-Index)\n",
    "plt.plot(valid_data['date'], Cros_4_df, label='Croston', linestyle=':', color='red')\n",
    "\n",
    "# Plot für Arima_4\n",
    "plt.plot(valid_data['date'], Arima_4, label='ARIMA', linestyle='-', color='orange')\n",
    "\n",
    "# Plot für Sarima_4\n",
    "plt.plot(valid_data['date'], Sarima_4, label='SARIMA', linestyle=(0, (5, 2)), color='purple')\n",
    "\n",
    "# Plot für valid_data\n",
    "plt.plot(valid_data['date'], valid_data['value'], label='Testdaten', color='darkgreen', linestyle='--')\n",
    "\n",
    "# Legende anzeigen außerhalb des Plots (rechts)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Absatz [Stück]')\n",
    "\n",
    "# Gitter anzeigen\n",
    "plt.grid(True)\n",
    "\n",
    "# Drehung der x-Achsenbeschriftungen um 45 Grad\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.savefig('LinReg, Cros, Arima, Satima_train_zus.png', bbox_inches='tight')  # Exportieren als PNG\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setzen der Schriftgröße\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Setzen der Größe des Plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Umwandlung von Cros_5 und LinReg_5 in Pandas-Datenrahmen mit dem Index von Arima_5\n",
    "Cros_5_df = pd.DataFrame({'Cros_5': Cros_5}, index=Arima_5.index)\n",
    "LinReg_5_df = pd.DataFrame({'LinReg_5': linReg_5}, index=Arima_5.index)\n",
    "\n",
    "# Plot für LinReg_5 (mit Arima-Index)\n",
    "plt.plot(test_data['date'], LinReg_5_df, label='LinReg', linestyle='-.', color='blue')\n",
    "\n",
    "# Plot für Cros_5 (mit Arima-Index)\n",
    "plt.plot(test_data['date'], Cros_5_df, label='Croston', linestyle=':', color='red')\n",
    "\n",
    "# Plot für Arima_5\n",
    "plt.plot(test_data['date'], Arima_5, label='ARIMA', linestyle='-', color='orange')\n",
    "\n",
    "# Plot für Sarima_5\n",
    "plt.plot(test_data['date'], Sarima_5, label='SARIMA', linestyle=(0,(5, 2)), color='purple')\n",
    "\n",
    "# Plot für valid_data\n",
    "# Filtern der letzten 20% der Trainingsdaten\n",
    "train_data_last_20_percent = train_valid_data.tail(int(len(train_valid_data) * 0.2))\n",
    "\n",
    "plt.plot(train_data_last_20_percent['date'], train_data_last_20_percent['value'], label='Trainingsdaten', color='black', linestyle='--')\n",
    "\n",
    "\n",
    "# Plot Testdaten\n",
    "plt.plot(test_data['date'], test_data['value'], label='Validierungsdaten', color='darkgreen', linestyle='--')\n",
    "\n",
    "# Legende anzeigen außerhalb des Plots (rechts)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Absatz [Stück]')\n",
    "\n",
    "# Drehung der x-Achsenbeschriftungen um 45 Grad\n",
    "plt.xticks(rotation=45)\n",
    "# Gitter anzeigen\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.savefig('LinReg, Cros, Arima, Satima_test_kom.png', bbox_inches='tight')  # Exportieren als PNG\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setzen der Schriftgröße\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Setzen der Größe des Plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Umwandlung von Cros_5 und LinReg_5 in Pandas-Datenrahmen mit dem Index von Arima_5\n",
    "Cros_5_df = pd.DataFrame({'Cros_5': Cros_5}, index=Arima_5.index)\n",
    "LinReg_5_df = pd.DataFrame({'LinReg_5': linReg_5}, index=Arima_5.index)\n",
    "\n",
    "# Plot für LinReg_5 (mit Arima-Index)\n",
    "plt.plot(test_data['date'], LinReg_5_df, label='LinReg')\n",
    "\n",
    "# Plot für Cros_5 (mit Arima-Index)\n",
    "plt.plot(test_data['date'], Cros_5_df, label='Croston')\n",
    "\n",
    "# Plot für Arima_5\n",
    "plt.plot(test_data['date'], Arima_5, label='ARIMA')\n",
    "\n",
    "# Plot für Sarima_5\n",
    "plt.plot(test_data['date'], Sarima_5, label='SARIMA')\n",
    "\n",
    "# Plot für valid_data\n",
    "# plt.plot(train_valid_data['date'], train_valid_data['value'], label='Validierungsdaten', color='black', linestyle='--')\n",
    "plt.plot(test_data['date'], test_data['value'], label='Trainingsdaten', color='darkgreen', linestyle='--')\n",
    "\n",
    "# Legende anzeigen außerhalb des Plots (rechts)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Absatz [Stück]')\n",
    "\n",
    "\n",
    "\n",
    "# Gitter anzeigen\n",
    "plt.grid(True)\n",
    "plt.savefig('LinReg, Cros, Arima, Satima_test_zus.png', bbox_inches='tight')  # Exportieren als PNG\n",
    "\n",
    "# Drehung der x-Achsenbeschriftungen um 45 Grad\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "81b53693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setzen der Schriftgröße\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Setzen der Größe des Plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot für LinReg_4 (mit Arima-Index)\n",
    "plt.plot(valid_data['date'], FBPu_4, label='Prophet-u')\n",
    "\n",
    "# Plot für HWAdd_4\n",
    "plt.plot(valid_data['date'], HWAdd_4, label='H-W_add')\n",
    "\n",
    "# Plot für HWMul_4\n",
    "plt.plot(valid_data['date'], HWMul_4, label='H-W_mul')\n",
    "\n",
    "# Plot für valid_data\n",
    "plt.plot(valid_data['date'], valid_data['value'], label='Validierungsdaten', color='darkgreen', linestyle='--')\n",
    "\n",
    "# Plot für Trainingsdaten\n",
    "plt.plot(train_data['date'], train_data['value'], label='Trainingsdaten', color='black', linestyle='--')\n",
    "\n",
    "# Legende anzeigen außerhalb des Plots (rechts)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Absatz [Stück]')\n",
    "\n",
    "# Gitter anzeigen\n",
    "plt.grid(True)\n",
    "\n",
    "# Drehung der x-Achsenbeschriftungen um 45 Grad\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('FB, HW_train_kom.png', bbox_inches='tight')  # Exportieren als PNG\n",
    "# Plot anzeigen\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Setzen der Schriftgröße\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Setzen der Größe des Plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot für LinReg_4 (mit Arima-Index)\n",
    "plt.plot(valid_data['date'], FBPu_4, label='Prophet-u', linestyle='-.', color='blue')\n",
    "\n",
    "# Plot für HWAdd_4\n",
    "plt.plot(valid_data['date'], HWAdd_4, label='H-W_add', linestyle=':', color='red')\n",
    "\n",
    "# Plot für HWMul_4\n",
    "plt.plot(valid_data['date'], HWMul_4, label='H-W_mul', linestyle='-', color='orange')\n",
    "\n",
    "# Plot für valid_data\n",
    "plt.plot(valid_data['date'], valid_data['value'], label='Testdaten', color='darkgreen', linestyle='--')\n",
    "\n",
    "# Legende anzeigen außerhalb des Plots (rechts)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Absatz [Stück]')\n",
    "\n",
    "# Gitter anzeigen\n",
    "plt.grid(True)\n",
    "\n",
    "# Drehung der x-Achsenbeschriftungen um 45 Grad\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.savefig('FB, HW_train_zus.png', bbox_inches='tight')  # Exportieren als PNG\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setzen der Schriftgröße\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Setzen der Größe des Plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot für LinReg_5 (mit Arima-Index)\n",
    "plt.plot(test_data['date'], FBPu_5, label='Prophet-u', linestyle='-.', color='blue')\n",
    "\n",
    "# Plot für HWAdd_5\n",
    "plt.plot(test_data['date'], HWAdd_5, label='H-W_add', linestyle=':', color='red')\n",
    "\n",
    "# Plot für HWMul_5\n",
    "plt.plot(test_data['date'], HWMul_5, label='H-W_mul', linestyle='-', color='orange')\n",
    "\n",
    "# Plot für valid_data\n",
    "# Filtern der letzten 20% der Trainingsdaten\n",
    "train_data_last_20_percent = train_valid_data.tail(int(len(train_valid_data) * 0.2))\n",
    "\n",
    "plt.plot(train_data_last_20_percent['date'], train_data_last_20_percent['value'], label='Trainingsdaten', color='black', linestyle='--')\n",
    "\n",
    "\n",
    "# Plot Testdaten\n",
    "plt.plot(test_data['date'], test_data['value'], label='Validierungsdaten', color='darkgreen', linestyle='--')\n",
    "\n",
    "# Legende anzeigen außerhalb des Plots (rechts)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Absatz [Stück]')\n",
    "\n",
    "# Drehung der x-Achsenbeschriftungen um 45 Grad\n",
    "plt.xticks(rotation=45)\n",
    "# Gitter anzeigen\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('FB, HW_test_kom.png', bbox_inches='tight')  # Exportieren als PNG\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setzen der Schriftgröße\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Setzen der Größe des Plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "\n",
    "# Plot für LinReg_5 (mit Arima-Index)\n",
    "plt.plot(test_data['date'], FBPu_5, label='Prophet-u')\n",
    "\n",
    "# Plot für HWAdd_5\n",
    "plt.plot(test_data['date'], HWAdd_5, label='H-W_add')\n",
    "\n",
    "# Plot für HWMul_5\n",
    "plt.plot(test_data['date'], HWMul_5, label='H-W_mul')\n",
    "\n",
    "# Plot für valid_data\n",
    "# plt.plot(train_valid_data['date'], train_valid_data['value'], label='Validierungsdaten', color='black', linestyle='--')\n",
    "plt.plot(test_data['date'], test_data['value'], label='Trainingsdaten', color='darkgreen', linestyle='--')\n",
    "\n",
    "# Legende anzeigen außerhalb des Plots (rechts)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Absatz [Stück]')\n",
    "\n",
    "\n",
    "\n",
    "# Gitter anzeigen\n",
    "plt.grid(True)\n",
    "\n",
    "# Drehung der x-Achsenbeschriftungen um 45 Grad\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.savefig('FB, HW_test_zus.png', bbox_inches='tight')  # Exportieren als PNG\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9784a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setzen der Schriftgröße\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Setzen der Größe des Plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot für LSTM_4\n",
    "plt.plot(valid_korr['date'],LSTM_4, label='LSTM')\n",
    "\n",
    "# Plot für RF_4\n",
    "plt.plot(valid_korr['date'],RF_4, label='RF')\n",
    "\n",
    "# Plot für SVM_4\n",
    "plt.plot(valid_korr['date'],SVM_4, label='SVR')\n",
    "\n",
    "# Plot für FBPm_4\n",
    "plt.plot(valid_korr['date'],FBPm_4, label='Prophet-m')\n",
    "\n",
    "# Plot für LSTM_b_4\n",
    "plt.plot(valid_korr['date'],LSTM_b_4, label='LSTM_BFS')\n",
    "\n",
    "# Plot für valid_data\n",
    "plt.plot(valid_korr['date'], valid_korr['value'], label='Validierungsdaten', color='darkgreen', linestyle='--')\n",
    "\n",
    "# Plot für Trainingsdaten\n",
    "plt.plot(train_korr['date'], train_korr['value'], label='Trainingsdaten', color='black', linestyle='--')\n",
    "\n",
    "# Legende anzeigen außerhalb des Plots (rechts)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Absatz [Stück]')\n",
    "\n",
    "# Gitter anzeigen\n",
    "plt.grid(True)\n",
    "\n",
    "# Drehung der x-Achsenbeschriftungen um 45 Grad\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('multi_train_kom.png', bbox_inches='tight')  # Exportieren als PNG\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Setzen der Schriftgröße\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Setzen der Größe des Plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot für LSTM_4\n",
    "plt.plot(valid_korr['date'],LSTM_4, label='LSTM', linestyle='-.', color='blue')\n",
    "\n",
    "# Plot für RF_4\n",
    "plt.plot(valid_korr['date'],RF_4, label='RF', linestyle=':', color='red')\n",
    "\n",
    "# Plot für SVM_4\n",
    "plt.plot(valid_korr['date'],SVM_4, label='SVR', linestyle='-', color='orange')\n",
    "\n",
    "# Plot für FBPm_4\n",
    "plt.plot(valid_korr['date'],FBPm_4, label='Prophet-m', linestyle=(0,(10,2)), color='purple')\n",
    "\n",
    "# Plot für LSTM_b_4\n",
    "plt.plot(valid_korr['date'],LSTM_b_4, label='LSTM_BFS', linestyle=(0,(2,5)), color='brown')\n",
    "\n",
    "# Plot für valid_data\n",
    "plt.plot(valid_korr['date'], valid_korr['value'], label='Testdaten', color='darkgreen', linestyle='--')\n",
    "\n",
    "# Legende anzeigen außerhalb des Plots (rechts)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Absatz [Stück]')\n",
    "\n",
    "# Gitter anzeigen\n",
    "plt.grid(True)\n",
    "\n",
    "# Drehung der x-Achsenbeschriftungen um 45 Grad\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('multi_train_zus.png', bbox_inches='tight')  # Exportieren als PNG\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setzen der Schriftgröße\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Setzen der Größe des Plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot für LSTM_4\n",
    "plt.plot(test_korr['date'],LSTM_5, label='LSTM', linestyle='-.', color='blue')\n",
    "\n",
    "# Plot für RF_4\n",
    "plt.plot(test_korr['date'],RF_5, label='RF', linestyle=':', color='red')\n",
    "\n",
    "# Plot für SVM_4\n",
    "plt.plot(test_korr['date'],SVM_5, label='SVR', linestyle='-', color='orange')\n",
    "\n",
    "# Plot für FBPm_4\n",
    "plt.plot(test_korr['date'],FBPm_5, label='Prophet-m', linestyle=(0,(10,2)), color='purple')\n",
    "\n",
    "# Plot für LSTM_b_4\n",
    "plt.plot(test_korr['date'],LSTM_b_5, label='LSTM_BFS', linestyle=(0,(2,5)), color='brown')\n",
    "\n",
    "# Plot für valid_data\n",
    "# Filtern der letzten 20% der Trainingsdaten\n",
    "train_korr_last_20_percent = train_valid_korr.tail(int(len(train_valid_korr) * 0.2))\n",
    "\n",
    "plt.plot(train_korr_last_20_percent['date'], train_korr_last_20_percent['value'], label='Trainingsdaten', color='black', linestyle='--')\n",
    "\n",
    "\n",
    "# Plot Testdaten\n",
    "plt.plot(test_korr['date'], test_korr['value'], label='Validierungsdaten', color='darkgreen', linestyle='--')\n",
    "\n",
    "# Legende anzeigen außerhalb des Plots (rechts)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Absatz [Stück]')\n",
    "\n",
    "# Drehung der x-Achsenbeschriftungen um 45 Grad\n",
    "plt.xticks(rotation=45)\n",
    "# Gitter anzeigen\n",
    "plt.grid(True)\n",
    "plt.savefig('multi_test_kom.png', bbox_inches='tight')  # Exportieren als PNG\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setzen der Schriftgröße\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Setzen der Größe des Plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot für LSTM_4\n",
    "plt.plot(test_korr['date'],LSTM_5, label='LSTM')\n",
    "\n",
    "# Plot für RF_4\n",
    "plt.plot(test_korr['date'],RF_5, label='RF')\n",
    "\n",
    "# Plot für SVM_4\n",
    "plt.plot(test_korr['date'],SVM_5, label='SVR')\n",
    "\n",
    "# Plot für FBPm_4\n",
    "plt.plot(test_korr['date'],FBPm_5, label='Prophet-m')\n",
    "\n",
    "# Plot für LSTM_b_4\n",
    "plt.plot(test_korr['date'],LSTM_b_5, label='LSTM_BFS')\n",
    "\n",
    "# Plot für valid_data\n",
    "# plt.plot(train_valid_data['date'], train_valid_data['value'], label='Validierungsdaten', color='black', linestyle='--')\n",
    "plt.plot(test_korr['date'], test_korr['value'], label='Trainingsdaten', color='darkgreen', linestyle='--')\n",
    "\n",
    "# Legende anzeigen außerhalb des Plots (rechts)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Absatz [Stück]')\n",
    "\n",
    "\n",
    "\n",
    "# Gitter anzeigen\n",
    "plt.grid(True)\n",
    "\n",
    "# Drehung der x-Achsenbeschriftungen um 45 Grad\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.savefig('multi_test_zus.png', bbox_inches='tight')  # Exportieren als PNG\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
